{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Detecting Tables using Pytorch and Lightning This repository contains the code for detecting tables in PDF documents by using the PyTorch and the Lightning framework. Warning This repository is still a work in progress and things might change drastically. Requirements The requirements for this repository can be found in the requirements.txt file in the root of respository. The requirements can be installed using the following command using pip: pip install -r requirements.txt Folder Structure of the Repository The repository folders are structured in the following way. - network - datamodules.py - models.py - trainers.py - transforms.py - utilities.py - utils - visualisers.py - requirements.txt The network folder contains all the files that deal for the creation, configuration, training and inference of PyTorch based models. The datamodules.py file contains everything related to the management of dataset for the training of the models. The models.py file contains all the PyTorch based models e.g. the RetinaNet. The trainers.py contains all the necessary assembly of functions to initialise the training of the models. More information about the training of the models is provided below. Since we are dealing with object detection models, the runtime transforms that might be required to apply to the images need to be applied to the annotations i.e. bounding boxes as well. These special transforms are present in the transforms.py file. The utilities.py file contains helper utilities for the setup of the models. The utils folder provides some utilities that might be needed to interface with the network. visualisers.py provides some functions for the purposes of visualising the detections made by the network. requirements.txt file contains the required Python packages to run the code in this repository.","title":"Home"},{"location":"#detecting-tables-using-pytorch-and-lightning","text":"This repository contains the code for detecting tables in PDF documents by using the PyTorch and the Lightning framework. Warning This repository is still a work in progress and things might change drastically.","title":"Detecting Tables using Pytorch and Lightning"},{"location":"#requirements","text":"The requirements for this repository can be found in the requirements.txt file in the root of respository. The requirements can be installed using the following command using pip: pip install -r requirements.txt","title":"Requirements"},{"location":"#folder-structure-of-the-repository","text":"The repository folders are structured in the following way. - network - datamodules.py - models.py - trainers.py - transforms.py - utilities.py - utils - visualisers.py - requirements.txt The network folder contains all the files that deal for the creation, configuration, training and inference of PyTorch based models. The datamodules.py file contains everything related to the management of dataset for the training of the models. The models.py file contains all the PyTorch based models e.g. the RetinaNet. The trainers.py contains all the necessary assembly of functions to initialise the training of the models. More information about the training of the models is provided below. Since we are dealing with object detection models, the runtime transforms that might be required to apply to the images need to be applied to the annotations i.e. bounding boxes as well. These special transforms are present in the transforms.py file. The utilities.py file contains helper utilities for the setup of the models. The utils folder provides some utilities that might be needed to interface with the network. visualisers.py provides some functions for the purposes of visualising the detections made by the network. requirements.txt file contains the required Python packages to run the code in this repository.","title":"Folder Structure of the Repository"},{"location":"guides/training/","text":"How to Train the Networks The repository will contain the code to train multiple networks on the dataset. The trainers.py file can be used for the purposes of training the available models. Currently only the default PyTorch RetinaNet is available for the training purposes. Known Issues Following are some known issues with the code: Negative examples do not work while training. They increase the loss to infinity from which the network never recovers. Tested On The code has been tested to run on: macOS 12.4 Monterey on Intel Macbook Pro 2019 Google Colab","title":"Overview"},{"location":"guides/training/#how-to-train-the-networks","text":"The repository will contain the code to train multiple networks on the dataset. The trainers.py file can be used for the purposes of training the available models. Currently only the default PyTorch RetinaNet is available for the training purposes.","title":"How to Train the Networks"},{"location":"guides/training/#known-issues","text":"Following are some known issues with the code: Negative examples do not work while training. They increase the loss to infinity from which the network never recovers.","title":"Known Issues"},{"location":"guides/training/#tested-on","text":"The code has been tested to run on: macOS 12.4 Monterey on Intel Macbook Pro 2019 Google Colab","title":"Tested On"},{"location":"guides/training/checkpoints/","text":"About the Checkpoints The checkpoint is a terminology taken from the PyTorch Lightning framework. It is a file that contains the state of the model. Checkpoints are saved successively throughout the training process. The benefit of keeping checkpoints is that we can keep track and save the model state that performed the best since training of the network is an iterative process. Following checkpoints are saved through the training process: Checkpoint for the last epoch Checkpoint for the best Evaluation Average IoU Checkpoint for the best Evalutaion Precision (at 75% Confidence Score and 90% IoU) Checkpoint for the best Evaluation Recall (at 75% Confidence Score and 90% IoU)","title":"Checkpoints"},{"location":"guides/training/checkpoints/#about-the-checkpoints","text":"The checkpoint is a terminology taken from the PyTorch Lightning framework. It is a file that contains the state of the model. Checkpoints are saved successively throughout the training process. The benefit of keeping checkpoints is that we can keep track and save the model state that performed the best since training of the network is an iterative process. Following checkpoints are saved through the training process: Checkpoint for the last epoch Checkpoint for the best Evaluation Average IoU Checkpoint for the best Evalutaion Precision (at 75% Confidence Score and 90% IoU) Checkpoint for the best Evaluation Recall (at 75% Confidence Score and 90% IoU)","title":"About the Checkpoints"},{"location":"guides/training/command/","text":"Training Command This section will show how to initiate the training and what parameters are offered in the training command. Breakdown of the Training Command The trainer.py file accepts the following arguments when initialising the training: dataset_path : ( Type : String, Default : ./ ) This specifies the path to the dataset. More information about the format of the dataset folder is mentioned in the following sections. checkpoint_path : ( Type : String, Default : ./ ) This is used to specify the path where the checkpoints should be saved. More information about this is mentioned in the following sections. epochs : ( Type : Postive Integer, Default : 50 ) This specifies the number of epochs to train for. config_path : ( Type : String, Default : None ) This is the path to file that contains all of the training configuration settings. Please refer to the Experiment Configurations for more information. config_name : ( Type : String, Default : None ) This specifies the name of configuration setting to use from the file the file that contains all of the training configuration settings specified above. Please refer to the Experiment Configurations for more information. subset : ( Type : Positive Integer, Default : None ) This specifies if there is a need to take a random subset of the dataset. This can be useful initially when building the model or debugging to remove the bugs. It will be much faster to train on a small subset of the data to detect bugs. Specify None to not create subset of the data. log_name : ( Type : String, Default : None ) This specifies the name of the project for logging metrics to the Weights and Biases account. More information about metrics and logging are mentioned in the following sections. Specify None to not log anything to Weights and Biases. num_classes : ( Type : Positive Integer, Default : 2 ) This specifies the number of classes in the dataset. For this particular example since there is only one object to be detected this parameter should be specified as 2. Note that 2 classes are specified instead of 1 because that is the PyTorch convention. One class represents the background and one class represents the object to detect i.e. Table. Example of the Training Command An example of training command is as follows. It trains the RetinaNet model for 50 epochs, with only 10 images randomly selected from the dataset, 8 of which will be used as the training images and 2 as evaluation images. It also provides the path for the dataset and the path to save the checkpoint. python network/trainers.py --dataset_path \"datasets/original/\" --epochs 50 --checkpoint_path \"checkpoint/\" --config_path \"experiment-configs.yml\" --config_name \"vanilla_fasterrcnn_v2_1\" --log_name \"test-test\"","title":"Training Command"},{"location":"guides/training/command/#training-command","text":"This section will show how to initiate the training and what parameters are offered in the training command.","title":"Training Command"},{"location":"guides/training/command/#breakdown-of-the-training-command","text":"The trainer.py file accepts the following arguments when initialising the training: dataset_path : ( Type : String, Default : ./ ) This specifies the path to the dataset. More information about the format of the dataset folder is mentioned in the following sections. checkpoint_path : ( Type : String, Default : ./ ) This is used to specify the path where the checkpoints should be saved. More information about this is mentioned in the following sections. epochs : ( Type : Postive Integer, Default : 50 ) This specifies the number of epochs to train for. config_path : ( Type : String, Default : None ) This is the path to file that contains all of the training configuration settings. Please refer to the Experiment Configurations for more information. config_name : ( Type : String, Default : None ) This specifies the name of configuration setting to use from the file the file that contains all of the training configuration settings specified above. Please refer to the Experiment Configurations for more information. subset : ( Type : Positive Integer, Default : None ) This specifies if there is a need to take a random subset of the dataset. This can be useful initially when building the model or debugging to remove the bugs. It will be much faster to train on a small subset of the data to detect bugs. Specify None to not create subset of the data. log_name : ( Type : String, Default : None ) This specifies the name of the project for logging metrics to the Weights and Biases account. More information about metrics and logging are mentioned in the following sections. Specify None to not log anything to Weights and Biases. num_classes : ( Type : Positive Integer, Default : 2 ) This specifies the number of classes in the dataset. For this particular example since there is only one object to be detected this parameter should be specified as 2. Note that 2 classes are specified instead of 1 because that is the PyTorch convention. One class represents the background and one class represents the object to detect i.e. Table.","title":"Breakdown of the Training Command"},{"location":"guides/training/command/#example-of-the-training-command","text":"An example of training command is as follows. It trains the RetinaNet model for 50 epochs, with only 10 images randomly selected from the dataset, 8 of which will be used as the training images and 2 as evaluation images. It also provides the path for the dataset and the path to save the checkpoint. python network/trainers.py --dataset_path \"datasets/original/\" --epochs 50 --checkpoint_path \"checkpoint/\" --config_path \"experiment-configs.yml\" --config_name \"vanilla_fasterrcnn_v2_1\" --log_name \"test-test\"","title":"Example of the Training Command"},{"location":"guides/training/configs/","text":"Experiment Configurations This section will give information about the usage of the experiment configurations that is required for training of the networks. Why do we need an experiment configuration file? While developing a machine learning application, it can be very hard to keep track of all the parameters that we change trying to attain higher accuracy. This case is no different. There are a number of object detection models that can be used for detection of tables in the PDF documents, each of these models will have their own set of hyperparameters that need to be tuned to achieve optimal performance. Keeping this in mind, an separate yaml file must be created (provided with this repository) that includes the parameters, models and data modules that were used while training. This will also help other people in reproducing the work that was done previously. Breakdown of an Experiment Configuration One example of such configuration will be mentioned below, the rest can be developed easily by following the same pattern. vanilla: name: \"vanilla\" model: \"VanillaRetinaNet\" datamodule: \"TableDatasetModule\" learning_rate: 1e-5 batch_size: 2 train_eval_split: 0.8 As it can be seen from the configuration file, the formatting is in YAML. The header of the configuration (in this case \"vanilla\") specifies the name of the configuration. This name will need to be specified in the training command when starting a trainig run. Following are the parameters that need to supplied in an experiment configuration: name : This is the name of the configuration, this name will also be appended as a tag in the Weights and Biases run. See the Metrics for more information. model : This specifies the name of the model to use. The name of the model should exactly the same as name of the class that implements the model in the models.py file in the networks folder. datamodule : This specifies the datamodule to use for the training run. As for the models, the name should match the name of the class that implements the required datamodule in the datamodules.py file in the networks folder. learning_rate : This will specify the learning rate in the training run. batch_size : This parameter specifies the number of images that will be fed to the model in each iteration during the training phase. train_eval_split : This is a float number between the range of 0 - 1 that specifies the split of the dataset between training and evaluation set. The number specified is for the size of the training set. For example: 0.8 will mean 80% of the dataset to use as training set and 20% of the dataset to use as evaluation set. All of these configurations will be printed on the console when the training run starts so that it can be validated if the correct values were read.","title":"Experiment Configurations"},{"location":"guides/training/configs/#experiment-configurations","text":"This section will give information about the usage of the experiment configurations that is required for training of the networks.","title":"Experiment Configurations"},{"location":"guides/training/configs/#why-do-we-need-an-experiment-configuration-file","text":"While developing a machine learning application, it can be very hard to keep track of all the parameters that we change trying to attain higher accuracy. This case is no different. There are a number of object detection models that can be used for detection of tables in the PDF documents, each of these models will have their own set of hyperparameters that need to be tuned to achieve optimal performance. Keeping this in mind, an separate yaml file must be created (provided with this repository) that includes the parameters, models and data modules that were used while training. This will also help other people in reproducing the work that was done previously.","title":"Why do we need an experiment configuration file?"},{"location":"guides/training/configs/#breakdown-of-an-experiment-configuration","text":"One example of such configuration will be mentioned below, the rest can be developed easily by following the same pattern. vanilla: name: \"vanilla\" model: \"VanillaRetinaNet\" datamodule: \"TableDatasetModule\" learning_rate: 1e-5 batch_size: 2 train_eval_split: 0.8 As it can be seen from the configuration file, the formatting is in YAML. The header of the configuration (in this case \"vanilla\") specifies the name of the configuration. This name will need to be specified in the training command when starting a trainig run. Following are the parameters that need to supplied in an experiment configuration: name : This is the name of the configuration, this name will also be appended as a tag in the Weights and Biases run. See the Metrics for more information. model : This specifies the name of the model to use. The name of the model should exactly the same as name of the class that implements the model in the models.py file in the networks folder. datamodule : This specifies the datamodule to use for the training run. As for the models, the name should match the name of the class that implements the required datamodule in the datamodules.py file in the networks folder. learning_rate : This will specify the learning rate in the training run. batch_size : This parameter specifies the number of images that will be fed to the model in each iteration during the training phase. train_eval_split : This is a float number between the range of 0 - 1 that specifies the split of the dataset between training and evaluation set. The number specified is for the size of the training set. For example: 0.8 will mean 80% of the dataset to use as training set and 20% of the dataset to use as evaluation set. All of these configurations will be printed on the console when the training run starts so that it can be validated if the correct values were read.","title":"Breakdown of an Experiment Configuration"},{"location":"guides/training/dataset/","text":"About the Dataset The way the code has been setup, it requires the dataset folder to be in a very specific format. The format is mentioned below. Folder Structure of Dataset The folder structure should be the following. - dataset - images - annotations.csv - classes.csv Important The name of the files and the folder should be exactly like mentioned here. This is because the names are hard coded so that only the path to root of the dataset folder needs to supplied when training. The headers shown in description below is just for information and should not be included in the CSV files themselves. images : A folder that contains all the images. annotations.csv : Annotations CSV file that contains the annotations in the following format: image_id,x1,y1,x2,y2,class_name Note that the image_id should just be name of the file in the images folder and not a path to the image. The dataset implementation in the code takes care of the path automatically. x1,y1 are the coordinates for the top-left of the box. x2,y2 are the coordinates for the bottom-right of the box. classes.csv : Class list CSV file that contains the list of class in the following format: class_name,class_id The object classes should start from 1 and not 0, 0 is reserved for background classes, if the dataset contains examples for the background class then 0 class_id can be used. Some Notes There are some particularities about the current implementation of dataset that need to be noted: There is no separate annotation file for the validation and training set. All the annotations should be mentioned in the annotations.csv file. Instead the Evaluation and Training split should be specified as an argument for the trainer.py file (See the 'How to Train the Networks' section). Negative examples are not allowed. These are images where there is no object to be detected. If such images are present, the loss is drived to infinity. This is a known issue currently for the code.","title":"About Datasets"},{"location":"guides/training/dataset/#about-the-dataset","text":"The way the code has been setup, it requires the dataset folder to be in a very specific format. The format is mentioned below.","title":"About the Dataset"},{"location":"guides/training/dataset/#folder-structure-of-dataset","text":"The folder structure should be the following. - dataset - images - annotations.csv - classes.csv Important The name of the files and the folder should be exactly like mentioned here. This is because the names are hard coded so that only the path to root of the dataset folder needs to supplied when training. The headers shown in description below is just for information and should not be included in the CSV files themselves. images : A folder that contains all the images. annotations.csv : Annotations CSV file that contains the annotations in the following format: image_id,x1,y1,x2,y2,class_name Note that the image_id should just be name of the file in the images folder and not a path to the image. The dataset implementation in the code takes care of the path automatically. x1,y1 are the coordinates for the top-left of the box. x2,y2 are the coordinates for the bottom-right of the box. classes.csv : Class list CSV file that contains the list of class in the following format: class_name,class_id The object classes should start from 1 and not 0, 0 is reserved for background classes, if the dataset contains examples for the background class then 0 class_id can be used.","title":"Folder Structure of Dataset"},{"location":"guides/training/dataset/#some-notes","text":"There are some particularities about the current implementation of dataset that need to be noted: There is no separate annotation file for the validation and training set. All the annotations should be mentioned in the annotations.csv file. Instead the Evaluation and Training split should be specified as an argument for the trainer.py file (See the 'How to Train the Networks' section). Negative examples are not allowed. These are images where there is no object to be detected. If such images are present, the loss is drived to infinity. This is a known issue currently for the code.","title":"Some Notes"},{"location":"guides/training/metrics/","text":"About Logging and Evaluation Metrics Evaluation metrics are critical in order to evaluate the model performance. It is necessary to keep track if these metrics throughout the training process as well in order make decisions about hyperparameters for example. Weights and Biases Integration In order to keep track of the model performance throughout the training process, there is Weights and Biases integration built into the code. Weights and biases is an MLOps platform that allows us to keep track of Machine Learning experiements. In order to log values to the Weights and Biases dashboard, a project name needs to be specified when running the training script, see the 'How to Train the Networks' section for more information. In addition to specifying the name of the projects, an account at Weights and Biases is needed. After logging in, the secret key will be available at https://wandb.ai/authorize . Enter this key when prompted while running the training script. Logged Metrics Different metrics are logged during the training and the validation phases. These can be visualised on the Weights and Biases dashboard. During training During the training process, following quantities are logged. Classification Loss: This is loss that the network calculates when classifying the region of objects into classes. For our case there is only one class of interest i.e. Table hence this loss goes down fairly quickly during the training phase. Regression Loss: This represents the loss that is calculated when the network tries to draw the bounding boxes around the tables. For us this quantity is more representative of the network training success. Total Loss: This is just the sum of both the classification and regression loss and provides the overview of the network training performance. Mean Total Loss: This is just the total loss averaged over the entire epoch. During Evaluation During the evaluation of the dataset, following quantities are logged. Averaged IoU: During the evaluation phase, the network is asked to make bounding box predictions over an image. Those box predictions are taken and compared with the ground truth bounding boxes. An overlap is calculated using a measure called IoU (Intersection of Union). Precision: During the evaluation phase, the network tries its best to make predictions of where the tables are. The predictions are the bounding boxes. Each of these bounding boxes come with their confidence scores. We select some of these boxes with a certain confidence score threshold. In addition to the confidence score, we also compute the overlap of these boxes with the ground truths and only select the boxes with a certain overlap threshold. So after both of these thresholds applied, we will have final set of detected tables. Precision value will tell us how many of these detected tables were actually tables. Recall: The process of calculating the recall metric is very similar to precision as described above. After applying both the confidence score and IoU thresholds, we get a set of predicted detections for the table. Recall tells us how many of the tables were actually detected by the network.","title":"Metrics"},{"location":"guides/training/metrics/#about-logging-and-evaluation-metrics","text":"Evaluation metrics are critical in order to evaluate the model performance. It is necessary to keep track if these metrics throughout the training process as well in order make decisions about hyperparameters for example.","title":"About Logging and Evaluation Metrics"},{"location":"guides/training/metrics/#weights-and-biases-integration","text":"In order to keep track of the model performance throughout the training process, there is Weights and Biases integration built into the code. Weights and biases is an MLOps platform that allows us to keep track of Machine Learning experiements. In order to log values to the Weights and Biases dashboard, a project name needs to be specified when running the training script, see the 'How to Train the Networks' section for more information. In addition to specifying the name of the projects, an account at Weights and Biases is needed. After logging in, the secret key will be available at https://wandb.ai/authorize . Enter this key when prompted while running the training script.","title":"Weights and Biases Integration"},{"location":"guides/training/metrics/#logged-metrics","text":"Different metrics are logged during the training and the validation phases. These can be visualised on the Weights and Biases dashboard.","title":"Logged Metrics"},{"location":"guides/training/metrics/#during-training","text":"During the training process, following quantities are logged. Classification Loss: This is loss that the network calculates when classifying the region of objects into classes. For our case there is only one class of interest i.e. Table hence this loss goes down fairly quickly during the training phase. Regression Loss: This represents the loss that is calculated when the network tries to draw the bounding boxes around the tables. For us this quantity is more representative of the network training success. Total Loss: This is just the sum of both the classification and regression loss and provides the overview of the network training performance. Mean Total Loss: This is just the total loss averaged over the entire epoch.","title":"During training"},{"location":"guides/training/metrics/#during-evaluation","text":"During the evaluation of the dataset, following quantities are logged. Averaged IoU: During the evaluation phase, the network is asked to make bounding box predictions over an image. Those box predictions are taken and compared with the ground truth bounding boxes. An overlap is calculated using a measure called IoU (Intersection of Union). Precision: During the evaluation phase, the network tries its best to make predictions of where the tables are. The predictions are the bounding boxes. Each of these bounding boxes come with their confidence scores. We select some of these boxes with a certain confidence score threshold. In addition to the confidence score, we also compute the overlap of these boxes with the ground truths and only select the boxes with a certain overlap threshold. So after both of these thresholds applied, we will have final set of detected tables. Precision value will tell us how many of these detected tables were actually tables. Recall: The process of calculating the recall metric is very similar to precision as described above. After applying both the confidence score and IoU thresholds, we get a set of predicted detections for the table. Recall tells us how many of the tables were actually detected by the network.","title":"During Evaluation"},{"location":"tools/","text":"Overview The documentation is structured in a way to separate the documentation of the command-line and other utilities from the guides that incorporate the usage of these utilities. This section of the documentation will provide information about all of the utilities that are available to the end user. Please check the navigation bar to the left in order to identify the utility that you are looking for or you can also search the relevant keywords for the utility that you need.","title":"Overview"},{"location":"tools/#overview","text":"The documentation is structured in a way to separate the documentation of the command-line and other utilities from the guides that incorporate the usage of these utilities. This section of the documentation will provide information about all of the utilities that are available to the end user. Please check the navigation bar to the left in order to identify the utility that you are looking for or you can also search the relevant keywords for the utility that you need.","title":"Overview"},{"location":"tools/network/","text":"Overview The network tools are the set of utilities that will be used to interact with the Deep Learning Object Detection Models. The tools can be used to train the model, as well as to generate and visualise the results that the network is producing.","title":"Overview"},{"location":"tools/network/#overview","text":"The network tools are the set of utilities that will be used to interact with the Deep Learning Object Detection Models. The tools can be used to train the model, as well as to generate and visualise the results that the network is producing.","title":"Overview"},{"location":"tools/network/infer/","text":"Infer Tool A command-line tool for generating an output file i.e. CSV for all the bounding boxes found in all of the pdf files that are provided as input to this utility. Purpose This tool was developed in order to get the inference results from the model and save them as a CSV file so that the data can be extracted from the tables that were detected. The tool can either take in a single pdf as an input or it can take folder that contains multiple pdf files as an input. How it works The utility when provided with an appropriate model, the weights for the model and the an appropriate input pdf file, can detect the tables in the input pdf file and save the output as a CSV file. It takes the input file, converts it into an image in case of a pdf file, then passes that image through the specified model, gets the bounding boxes with confidence score higher than the threshold provided. It then collects all the coordinates of the detected tables into a list and saves it in a CSV file. An example of such a CSV file is shown below: filename,pageno,x1,y1,x2,y2 601fa1f389734.pdf,1,258,2891,2155,5165 601fa1f389734.pdf,1,244,5426,2107,6218 601fa1f389733.pdf,2,210,3268,3162,4738 601fa1f389733.pdf,2,2079,5861,3125,6346 601fa1f389733.pdf,2,177,5774,2042,6376 601fa1f389733.pdf,2,1899,4921,3177,5538 601fa1f389733.pdf,2,216,4897,1865,5516 601fa1f389733.pdf,2,220,1165,3135,2009 601fa1f389733.pdf,2,246,2317,3114,2998 The first column is the name of the pdf was that was provided, the second column contains the page number on which the table was found. The column 3 through 6 mention the coordinates of the table that was detected. 'x1' and 'y1' are the coorindates for the top-left corner and 'x2' and 'y2' shows the bottom-right coordinates of the bounding box for the table that was detected. If a page in PDF contains no table, according to the model, then it is not mentioned at all in the CSV file. Parameters The following parameters can be specified to run the command line utility. Required Parameters -t or --type : The type of file on which to perform the inference. Valid options are 'pdf', 'pdfs_folder', 'image' or 'images_folder'. To visualised a single pdf file 'pdf' should be specified. To visualise a folder of pdf file, 'pdfs_folder' should be specified. To visualise a single image, 'image' should be specified and lastly in order to visualise a folder of images, 'images_folder' should be specified. Please make sure to include the full path to the files, in case of individual image or pdf, when specifying the path parameter below. -p or --path : This is the path to the type of file that was specified above. -m or --model : This is the name of the model that should be used to generate the bounding boxes on the images provided. The name of the model should exactly match the name of the class that implements the model in the models.py file. -w or --weights : In this parameter, the path to the Pytorch Lightning checkpoint should be provided that contains the trained weights for the model that was specified above. Please make sure that the checkpoint provided belongs to the model that was specified above. -o or --output : This parameter will specify the path, where to save the CSV file. Please specify the name of the CSV file alongwith the extension '.csv' when specifying the folder in which to save the CSV file. Optional Parameters -h or --help : This will show instructions on how to use the utility. -c or --confidence : This parameter specifies the confidence threshold. It is a floating point number between 0 and 1. When the model makes predictions, it specfies how confident it is on those predicitons as well. This parameter will specify the cutoff, the predictions below this cutoff will not be considered. This is an optional parameter, if no value is specified then 0.75 will be taken. -d or --dpi : In order to detect tables in the PDF file. Each individual page is first converted to an image that will then be passed through the model. This parameter specifies the DPI for that rendered image. The higher this number, the higher the resolution of the rendered image will be but the process of inference will also will be slower. This parameter also needs to be selected in accordance with the utility that will extract the textual data from the table e.g. Camelot. This is because if the resolution is different, the coordinates of the detected table will also be different. Example An example of how to run the utility is provided below: python infer.py -t pdf -m VanillaRetinaNet -w ../misc/best-chkpnt-epoch=35.ckpt -c 0.80 -p ../misc/601fa1f389733.pdf -o ../misc/output.csv","title":"Infer"},{"location":"tools/network/infer/#infer-tool","text":"A command-line tool for generating an output file i.e. CSV for all the bounding boxes found in all of the pdf files that are provided as input to this utility.","title":"Infer Tool"},{"location":"tools/network/infer/#purpose","text":"This tool was developed in order to get the inference results from the model and save them as a CSV file so that the data can be extracted from the tables that were detected. The tool can either take in a single pdf as an input or it can take folder that contains multiple pdf files as an input.","title":"Purpose"},{"location":"tools/network/infer/#how-it-works","text":"The utility when provided with an appropriate model, the weights for the model and the an appropriate input pdf file, can detect the tables in the input pdf file and save the output as a CSV file. It takes the input file, converts it into an image in case of a pdf file, then passes that image through the specified model, gets the bounding boxes with confidence score higher than the threshold provided. It then collects all the coordinates of the detected tables into a list and saves it in a CSV file. An example of such a CSV file is shown below: filename,pageno,x1,y1,x2,y2 601fa1f389734.pdf,1,258,2891,2155,5165 601fa1f389734.pdf,1,244,5426,2107,6218 601fa1f389733.pdf,2,210,3268,3162,4738 601fa1f389733.pdf,2,2079,5861,3125,6346 601fa1f389733.pdf,2,177,5774,2042,6376 601fa1f389733.pdf,2,1899,4921,3177,5538 601fa1f389733.pdf,2,216,4897,1865,5516 601fa1f389733.pdf,2,220,1165,3135,2009 601fa1f389733.pdf,2,246,2317,3114,2998 The first column is the name of the pdf was that was provided, the second column contains the page number on which the table was found. The column 3 through 6 mention the coordinates of the table that was detected. 'x1' and 'y1' are the coorindates for the top-left corner and 'x2' and 'y2' shows the bottom-right coordinates of the bounding box for the table that was detected. If a page in PDF contains no table, according to the model, then it is not mentioned at all in the CSV file.","title":"How it works"},{"location":"tools/network/infer/#parameters","text":"The following parameters can be specified to run the command line utility.","title":"Parameters"},{"location":"tools/network/infer/#required-parameters","text":"-t or --type : The type of file on which to perform the inference. Valid options are 'pdf', 'pdfs_folder', 'image' or 'images_folder'. To visualised a single pdf file 'pdf' should be specified. To visualise a folder of pdf file, 'pdfs_folder' should be specified. To visualise a single image, 'image' should be specified and lastly in order to visualise a folder of images, 'images_folder' should be specified. Please make sure to include the full path to the files, in case of individual image or pdf, when specifying the path parameter below. -p or --path : This is the path to the type of file that was specified above. -m or --model : This is the name of the model that should be used to generate the bounding boxes on the images provided. The name of the model should exactly match the name of the class that implements the model in the models.py file. -w or --weights : In this parameter, the path to the Pytorch Lightning checkpoint should be provided that contains the trained weights for the model that was specified above. Please make sure that the checkpoint provided belongs to the model that was specified above. -o or --output : This parameter will specify the path, where to save the CSV file. Please specify the name of the CSV file alongwith the extension '.csv' when specifying the folder in which to save the CSV file.","title":"Required Parameters"},{"location":"tools/network/infer/#optional-parameters","text":"-h or --help : This will show instructions on how to use the utility. -c or --confidence : This parameter specifies the confidence threshold. It is a floating point number between 0 and 1. When the model makes predictions, it specfies how confident it is on those predicitons as well. This parameter will specify the cutoff, the predictions below this cutoff will not be considered. This is an optional parameter, if no value is specified then 0.75 will be taken. -d or --dpi : In order to detect tables in the PDF file. Each individual page is first converted to an image that will then be passed through the model. This parameter specifies the DPI for that rendered image. The higher this number, the higher the resolution of the rendered image will be but the process of inference will also will be slower. This parameter also needs to be selected in accordance with the utility that will extract the textual data from the table e.g. Camelot. This is because if the resolution is different, the coordinates of the detected table will also be different.","title":"Optional Parameters"},{"location":"tools/network/infer/#example","text":"An example of how to run the utility is provided below: python infer.py -t pdf -m VanillaRetinaNet -w ../misc/best-chkpnt-epoch=35.ckpt -c 0.80 -p ../misc/601fa1f389733.pdf -o ../misc/output.csv","title":"Example"},{"location":"tools/network/train/","text":"Train Tool This utility has not been implemented as of yet. The functionality for training the models is still only available in the trainers.py file in the network directory. Please take a look at this guide here for more information: Training the Networks .","title":"Train"},{"location":"tools/network/train/#train-tool","text":"This utility has not been implemented as of yet. The functionality for training the models is still only available in the trainers.py file in the network directory. Please take a look at this guide here for more information: Training the Networks .","title":"Train Tool"},{"location":"tools/network/visualise/","text":"Visualise Tool A command-line tool for visualing the outputs of the model given the trained checkpoint and an input image or a pdf file. Purpose This tool was developed in order to get a lens into what the network is doing before running the model in production and saving all of the results in a file. The purpose of this utility is to generate bounding boxes on a variety of input file types. This can be very useful for debugging purposes to see how well the network is detecting the tables. The goal was to create tool that can take a diverse range of input files, to that effect, this utility can visualise images, all of the images in a folder, pdf files and all of the pdf files in any given folder. How it works The utility when provided with an appropriate model, the weights for the model and the an appropriate input file, can show the user what the model is detecting by drawing bounding boxes on the detected tables. It takes the input file, converts it into an image in case of a pdf file, then passes that image through the specified model, gets the bounding boxes with confidence score higher than the threshold provided. It then draws the selected bounding boxes on the image and shows it on the screen. Each image will be shown one-by-one. Enter key needs to be pressed to go to the next image. Parameters The following parameters can be specified to run the command line utility. Required Parameters -t or --type : The type of file on which to perform the inference. Valid options are 'pdf', 'pdfs_folder', 'image' or 'images_folder'. To visualised a single pdf file 'pdf' should be specified. To visualise a folder of pdf file, 'pdfs_folder' should be specified. To visualise a single image, 'image' should be specified and lastly in order to visualise a folder of images, 'images_folder' should be specified. Please make sure to include the full path to the files, in case of individual image or pdf, when specifying the path parameter below. -p or --path : This is the path to the type of file that was specified above. -m or --model : This is the name of the model that should be used to generate the bounding boxes on the images provided. The name of the model should exactly match the name of the class that implements the model in the models.py file. -w or --weights : In this parameter, the path to the Pytorch Lightning checkpoint should be provided that contains the trained weights for the model that was specified above. Please make sure that the checkpoint provided belongs to the model that was specified above. Optional Parameters -h or --help : This will show instructions on how to use the utility. -c or --confidence : This parameter specifies the confidence threshold. It is a floating point number between 0 and 1. When the model makes predictions, it specfies how confident it is on those predicitons as well. This parameter will specify the cutoff, the predictions below this cutoff will not be considered. This is an optional parameter, if no value is specified then 0.75 will be taken. Example An example of how to run the utility is provided below: python visualise.py -t pdf -m VanillaRetinaNet -w ../misc/best-chkpnt-epoch=35.ckpt -c 0.80 -p ../misc/601fa1f389733.pdf","title":"Visualise"},{"location":"tools/network/visualise/#visualise-tool","text":"A command-line tool for visualing the outputs of the model given the trained checkpoint and an input image or a pdf file.","title":"Visualise Tool"},{"location":"tools/network/visualise/#purpose","text":"This tool was developed in order to get a lens into what the network is doing before running the model in production and saving all of the results in a file. The purpose of this utility is to generate bounding boxes on a variety of input file types. This can be very useful for debugging purposes to see how well the network is detecting the tables. The goal was to create tool that can take a diverse range of input files, to that effect, this utility can visualise images, all of the images in a folder, pdf files and all of the pdf files in any given folder.","title":"Purpose"},{"location":"tools/network/visualise/#how-it-works","text":"The utility when provided with an appropriate model, the weights for the model and the an appropriate input file, can show the user what the model is detecting by drawing bounding boxes on the detected tables. It takes the input file, converts it into an image in case of a pdf file, then passes that image through the specified model, gets the bounding boxes with confidence score higher than the threshold provided. It then draws the selected bounding boxes on the image and shows it on the screen. Each image will be shown one-by-one. Enter key needs to be pressed to go to the next image.","title":"How it works"},{"location":"tools/network/visualise/#parameters","text":"The following parameters can be specified to run the command line utility.","title":"Parameters"},{"location":"tools/network/visualise/#required-parameters","text":"-t or --type : The type of file on which to perform the inference. Valid options are 'pdf', 'pdfs_folder', 'image' or 'images_folder'. To visualised a single pdf file 'pdf' should be specified. To visualise a folder of pdf file, 'pdfs_folder' should be specified. To visualise a single image, 'image' should be specified and lastly in order to visualise a folder of images, 'images_folder' should be specified. Please make sure to include the full path to the files, in case of individual image or pdf, when specifying the path parameter below. -p or --path : This is the path to the type of file that was specified above. -m or --model : This is the name of the model that should be used to generate the bounding boxes on the images provided. The name of the model should exactly match the name of the class that implements the model in the models.py file. -w or --weights : In this parameter, the path to the Pytorch Lightning checkpoint should be provided that contains the trained weights for the model that was specified above. Please make sure that the checkpoint provided belongs to the model that was specified above.","title":"Required Parameters"},{"location":"tools/network/visualise/#optional-parameters","text":"-h or --help : This will show instructions on how to use the utility. -c or --confidence : This parameter specifies the confidence threshold. It is a floating point number between 0 and 1. When the model makes predictions, it specfies how confident it is on those predicitons as well. This parameter will specify the cutoff, the predictions below this cutoff will not be considered. This is an optional parameter, if no value is specified then 0.75 will be taken.","title":"Optional Parameters"},{"location":"tools/network/visualise/#example","text":"An example of how to run the utility is provided below: python visualise.py -t pdf -m VanillaRetinaNet -w ../misc/best-chkpnt-epoch=35.ckpt -c 0.80 -p ../misc/601fa1f389733.pdf","title":"Example"},{"location":"tools/utils/","text":"Utilities This section will provide an overview about the smaller scripts that were developed to facilite the process of training, testing and deploying the machine learning models.","title":"Overview"},{"location":"tools/utils/#utilities","text":"This section will provide an overview about the smaller scripts that were developed to facilite the process of training, testing and deploying the machine learning models.","title":"Utilities"},{"location":"tools/utils/data-house-keeper/","text":"Data House Keeper This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts. Purpose Data House Keeper helps in deleting extra files from a PascalVOC folder i.e. Extra XML and JPG files. It also can rename the files from the number specified so that all the XML and JPG files can be organised. Limitation The image files can only be in the JPG format. Limitation The name of the both counterparts (the image and its annotation XML) should have the same name for this script to work. This is the case for the files produced by labelimg, so if you're using that tool, it should be fine. How it works Once the script is started, it will go through the folder and find all the JPGs that have their XML files already present, it will match the names for doing so. If there are extra files present i.e. JPG files that have no matching XML files and vice versa, it will then ask if you want to delete the extra files, if the extra files are deleted only then it will proceed to the next step otherwise quit. If the extra files have been cleaned up (or there are no extra files) the script will ask if you want to rename the files, if you say yes then it will ask a number from which to start the numbering, this can be useful if you already have images in another folder and you just want to add more labelled data. Parameters Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above. Example An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"Data House Keeper"},{"location":"tools/utils/data-house-keeper/#data-house-keeper","text":"This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts.","title":"Data House Keeper"},{"location":"tools/utils/data-house-keeper/#purpose","text":"Data House Keeper helps in deleting extra files from a PascalVOC folder i.e. Extra XML and JPG files. It also can rename the files from the number specified so that all the XML and JPG files can be organised. Limitation The image files can only be in the JPG format. Limitation The name of the both counterparts (the image and its annotation XML) should have the same name for this script to work. This is the case for the files produced by labelimg, so if you're using that tool, it should be fine.","title":"Purpose"},{"location":"tools/utils/data-house-keeper/#how-it-works","text":"Once the script is started, it will go through the folder and find all the JPGs that have their XML files already present, it will match the names for doing so. If there are extra files present i.e. JPG files that have no matching XML files and vice versa, it will then ask if you want to delete the extra files, if the extra files are deleted only then it will proceed to the next step otherwise quit. If the extra files have been cleaned up (or there are no extra files) the script will ask if you want to rename the files, if you say yes then it will ask a number from which to start the numbering, this can be useful if you already have images in another folder and you just want to add more labelled data.","title":"How it works"},{"location":"tools/utils/data-house-keeper/#parameters","text":"Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above.","title":"Parameters"},{"location":"tools/utils/data-house-keeper/#example","text":"An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"Example"},{"location":"tools/utils/pascalvoc-to-csv/","text":"PascalVOC to CSV This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts. Purpose This script converts the image annotations/labellings from the PascalVOC (XML) format to CSV and writes them as a CSV file. Warning The script only reads the XML annotation files and converts them to a single CSV file for saving. It is essential that before running this script, that the folder has been cleaned up using the Data House Keeper script. How it works It will go through each XML file in the folder and get the respective labelled objects. Warning Since this script was purpose built for the table detection algorithm, it will set the labels of all the annotations to the same thing i.e. \"Table\". This script is not made for the case of multiple objects. It will assemble one big Pandas DataFrame that will contain the annotations from all of the XML files present. Finally, it will ask if you want to save the CSV file, if you say yes then it will ask for the name of the file. Please enter a valid name i.e. not empty to save the file. Do not enter the extension i.e. \".csv\" in the filename, that is understood. Parameters Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above. Example An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"PascalVOC to CSV"},{"location":"tools/utils/pascalvoc-to-csv/#pascalvoc-to-csv","text":"This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts.","title":"PascalVOC to CSV"},{"location":"tools/utils/pascalvoc-to-csv/#purpose","text":"This script converts the image annotations/labellings from the PascalVOC (XML) format to CSV and writes them as a CSV file. Warning The script only reads the XML annotation files and converts them to a single CSV file for saving. It is essential that before running this script, that the folder has been cleaned up using the Data House Keeper script.","title":"Purpose"},{"location":"tools/utils/pascalvoc-to-csv/#how-it-works","text":"It will go through each XML file in the folder and get the respective labelled objects. Warning Since this script was purpose built for the table detection algorithm, it will set the labels of all the annotations to the same thing i.e. \"Table\". This script is not made for the case of multiple objects. It will assemble one big Pandas DataFrame that will contain the annotations from all of the XML files present. Finally, it will ask if you want to save the CSV file, if you say yes then it will ask for the name of the file. Please enter a valid name i.e. not empty to save the file. Do not enter the extension i.e. \".csv\" in the filename, that is understood.","title":"How it works"},{"location":"tools/utils/pascalvoc-to-csv/#parameters","text":"Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above.","title":"Parameters"},{"location":"tools/utils/pascalvoc-to-csv/#example","text":"An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"Example"}]}