{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Detecting Tables using Pytorch and Lightning","text":"<p>Warning</p> <p>This repository is still a work in progress and things might change drastically.</p> <p>This repository contains the code for detecting tables in PDF documents by using the PyTorch and the Lightning framework. The following image is just an example of passing a PDF through one of the networks in this repository that is trained on detecting the tables. The red bounding-boxes show the areas in the image that the model has predicted as a table.</p> <p></p>"},{"location":"#setting-up-dependencies","title":"Setting up Dependencies","text":"<p>The requirements for this repository can be found in the <code>requirements.txt</code> file in the root of respository. The requirements can be installed using the following command using pip:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>In addition, a dockerfile is provided that can setup the repository in the correct fashion and take care of all the dependecies of the code. The image can be generated using the following command. <pre><code>docker build -t image_name .\n</code></pre></p>"},{"location":"#evaluation","title":"Evaluation","text":"<p>This repository and all the code produced in it were designed to create an end-to-end pipeline for extracting tabular information from PDF documents. As evaluation is a critical part of any experimentation, specific code has been created that can perform evaluations of different aspects of the pipeline and reproduce the reesults. If you are interested in running the evaluations for yourself please follow the guide on Reproducing Evaluation.</p>"},{"location":"#information-about-files","title":"Information about Files","text":"<p>The repository contains many Python files in multiple folders that help make the complete pipeline for extracting tables from PDF documents. This is done in order to make the code more organised and modular and only some of these files are designed to be used by the end-user. For more information about the utilities that are designed to be exposed to the end-user, please check the Guides or the Tools section.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#november-28-2022","title":"November 28, 2022","text":""},{"location":"changelog/#code","title":"Code","text":"<ul> <li>Added the PDF De-duplication utility.</li> <li>Files and Misc files separation between the network tools and the utilities. </li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Added the documentation for the PDF De-duplication utility.</li> <li>Updated the repository folder structure in the Github Readme and the homepage of the main documentation.</li> <li>Added a banner image in both the Github repository and the homepage of the main documentation.</li> </ul>"},{"location":"downloads/","title":"Downloads","text":"<p>This section will provide links to download files and folders necessary to run the scripts in this utility.</p>"},{"location":"downloads/#evaluation-data","title":"Evaluation Data","text":"<p>Download Link</p> <p>The evaluation data contains all the models and the data required for reproducing the evaluation results for the experiments performed as part of the thesis. The checkpoints for deep learning models are also included, there is no need to download the checkpoits below separately. The Reproduce Evaluation guide can be studied to use this evaluation data correctly.</p>"},{"location":"downloads/#checkpoints","title":"Checkpoints","text":"<p>Checkpoints are files that contains the trained weights of the networks (alongwith other information). Each model type has its own type checkpoints that will be generated during the training. Please find the relevant checkpoints below:</p>"},{"location":"downloads/#vanillafasterrcnnv2","title":"VanillaFasterRCNNV2","text":"<p>Download Link</p> <p>This PyTorch Lightning Checkpoint contains the weights for the best trained FasterRCNN v2 model. This models was selected as being the best at detecting tables during the evaluations performed.</p>"},{"location":"downloads/#vanillafasterrcnn","title":"VanillaFasterRCNN","text":"<p>Download Link</p> <p>This PyTorch Lightning Checkpoint contains the weights for the best trained FasterRCNN model. </p>"},{"location":"downloads/#vanillaretinanet","title":"VanillaRetinaNet","text":"<p>Download Link</p> <p>This PyTorch Lightning Checkpoint contains the weights for the best trained RetinaNet model. </p>"},{"location":"guides/","title":"Guides","text":"<p>Welcome to the guides section of this documentation, on the left side bar you will find all of the guides that are currently available for using the scripts in this repository in an organised way.</p>"},{"location":"guides/evaluation/","title":"How to reproduce evaluation results","text":"<p>As part of the master thesis, there were some crucial results generated that evaluated not only the object detector models but also the other parts of the pipeline as well. In order to encourage future work, it is important to establish baselines in the form of evaluation results achieved using current approaches. Keeping that in mind, special care has been taken so that the reproduction of the evaluation results is easy. This is done so using a makefile that accompanies the code.</p>"},{"location":"guides/evaluation/#tested-on","title":"Tested On","text":"<p>The code has been tested to run on:</p> <ul> <li>macOS 12.4 Monterey on Intel Macbook Pro 2019</li> <li>Windows 11 on Intel Core i5 using Docker</li> </ul>"},{"location":"guides/evaluation/reproduce/","title":"Running the Scripts to Reproduce Results","text":"<p>After setting up the repository and the environment for the code, the scipts to generate the evaluation results can be run. In total there are 4 scripts included, the details of which will be mentioned below.</p>"},{"location":"guides/evaluation/reproduce/#table-detection-evaluation","title":"Table Detection Evaluation","text":"<p>This script will run the deep learning models on the evaluation images, compile the results and show them. Since, for this task the deep learning model will be used on above 400 images, this process might take a long time on CPU. You can perform this evaluation in two different ways.</p>"},{"location":"guides/evaluation/reproduce/#using-docker","title":"Using Docker","text":"<p>The docker image needs to be setup before running this command. Please check the Setup section for that. <pre><code>docker run image_name make evaluation-detection\n</code></pre></p>"},{"location":"guides/evaluation/reproduce/#on-your-own-computer","title":"On Your Own Computer","text":"<p>The requirements need to be installed before running this command. Please check the Setup section for that. <pre><code>make evaluation-detection\n</code></pre></p>"},{"location":"guides/evaluation/reproduce/#table-classification-evaluation","title":"Table Classification Evaluation","text":"<p>This script will train the machine learning models for the table classification task, compile the results and show them. This evaluation can be performed in two different ways.</p>"},{"location":"guides/evaluation/reproduce/#using-docker_1","title":"Using Docker","text":"<p>The docker image needs to be setup before running this command. Please check the Setup section for that. <pre><code>docker run image_name make evaluation-classification\n</code></pre></p>"},{"location":"guides/evaluation/reproduce/#on-your-own-computer_1","title":"On Your Own Computer","text":"<p>The requirements need to be installed before running this command. Please check the Setup section for that. <pre><code>make evaluation-classification\n</code></pre></p>"},{"location":"guides/evaluation/reproduce/#complete-pipeline-evaluation","title":"Complete Pipeline Evaluation","text":"<p>This script will generate the evaluation results for the complete pipeline. Here, the deep learning model will also be used but the number of images will be a lot less as computer to Table Detection so it will not take as long to complete this evaluation. This evaluation can be performed in two different ways.</p>"},{"location":"guides/evaluation/reproduce/#using-docker_2","title":"Using Docker","text":"<p>The docker image needs to be setup before running this command. Please check the Setup section for that. <pre><code>docker run image_name make evaluation-complete\n</code></pre></p>"},{"location":"guides/evaluation/reproduce/#on-your-own-computer_2","title":"On Your Own Computer","text":"<p>The requirements need to be installed before running this command. Please check the Setup section for that. <pre><code>make evaluation-complete\n</code></pre></p>"},{"location":"guides/evaluation/reproduce/#run-all-evaluations-above","title":"Run All Evaluations Above","text":"<p>This will run all three evaluations mentioned above one-by-one and show the results from them all collectively. As the deep learning model will be used twice, this evaluation might take very long to run on CPU. As with other evaluations mentioned above, this can be run in two different ways.</p>"},{"location":"guides/evaluation/reproduce/#using-docker_3","title":"Using Docker","text":"<p>The docker image needs to be setup before running this command. Please check the Setup section for that. <pre><code>docker run image_name make evaluation-complete\n</code></pre></p>"},{"location":"guides/evaluation/reproduce/#on-your-own-computer_3","title":"On Your Own Computer","text":"<p>The requirements need to be installed before running this command. Please check the Setup section for that. <pre><code>make evaluation-complete\n</code></pre></p>"},{"location":"guides/evaluation/setup/","title":"Setting up repository for evaluation","text":"<p>Two major steps are required for setting up the code for evaluation:</p>"},{"location":"guides/evaluation/setup/#1-download-data","title":"1. Download Data","text":"<p>In order to correctly run the scripts for reproducing the evaluation results, the repository needs two things:</p> <ul> <li> <p>The latest code from the GitHub repository. The repository can be cloned.</p> </li> <li> <p>The evaluation data zip file which can be found in Downloads section. Due to its large size, the evaluation data could not be uploaded to GitHub.</p> </li> </ul> <p>Upon extraction of the zip file, a folder will be created with the name 'evaluation-data'. This folder needs to be copied as it is to the root of the cloned repository. The name shall also remain the same. The final folder structure must look as shown in image below.</p> <p></p> <p>The files for running the evaluation scripts have now been assembled.</p>"},{"location":"guides/evaluation/setup/#2-setup-enviroment","title":"2. Setup Enviroment","text":"<p>Since there are multiple dependencies that were used for the development of the application, these dependecies need to be also installed for the evaluation purposes. There are two ways to meet the requirements of running the evaluation.</p>"},{"location":"guides/evaluation/setup/#using-docker-recommended","title":"Using Docker (Recommended)","text":"<p>A dockerfile is supplied with the code that can setup a docker image with all the dependencies of the code met. This is the easiest way to make sure the code works correctly. You will need to have docker installed on your computer already. After that, the following command can be executed to setup the docker image.</p> <p><pre><code>docker build -t image_name .\n</code></pre> Please replace \"image_name\" with the name you want the docker image to take.</p>"},{"location":"guides/evaluation/setup/#by-installing-packages-on-your-own-computer","title":"By Installing Packages on Your Own Computer","text":"<p>If you would rather install the packages on your own computer, please run the following command. You will need to have Python and pip installed already.</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"guides/training/","title":"How to Train the Models","text":"<p>The repository will contain the code to train multiple networks on the dataset. The <code>trainers.py</code> file can be used for the purposes of training the available models. The default PyTorch object detector models are available for training. </p>"},{"location":"guides/training/#known-issues","title":"Known Issues","text":"<p>Following are some known issues with the code:</p> <ul> <li>Negative examples do not work while training. They increase the loss to infinity from which the network never recovers.</li> </ul>"},{"location":"guides/training/#tested-on","title":"Tested On","text":"<p>The code has been tested to run on:</p> <ul> <li>macOS 12.4 Monterey on Intel Macbook Pro 2019</li> <li>Google Colab</li> <li>Kaggle</li> </ul>"},{"location":"guides/training/checkpoints/","title":"About the Checkpoints","text":"<p>The checkpoint is a terminology taken from the PyTorch Lightning framework. It is a file that contains the state of the model. Checkpoints are saved successively throughout the training process. The benefit of keeping checkpoints is that we can keep track and save the model state that performed the best since training of the network is an iterative process. Following checkpoints are saved through the training process:</p> <ul> <li>Checkpoint for the last epoch</li> <li>Checkpoint for the best Evaluation Average IoU</li> <li>Checkpoint for the best Evalutaion Precision (at 75% Confidence Score and 90% IoU)</li> <li>Checkpoint for the best Evaluation Recall (at 75% Confidence Score and 90% IoU)</li> </ul>"},{"location":"guides/training/command/","title":"Training Command","text":"<p>This section will show how to initiate the training and what parameters are offered in the training command.</p>"},{"location":"guides/training/command/#breakdown-of-the-training-command","title":"Breakdown of the Training Command","text":"<p>The <code>trainer.py</code> file accepts the following arguments when initialising the training:</p> <ul> <li> <p><code>dataset_path</code>:   (Type: String,    Default: <code>./</code>)   This specifies the path to the dataset. More information about the format of the dataset folder is mentioned in the following sections.</p> </li> <li> <p><code>checkpoint_path</code>:    (Type: String,    Default: <code>./</code>)   This is used to specify the path where the checkpoints should be saved. More information about this is mentioned in the following sections.</p> </li> <li> <p><code>epochs</code>:   (Type: Postive Integer,    Default: <code>50</code>)   This specifies the number of epochs to train for.</p> </li> <li> <p><code>config_path</code>:   (Type: String,    Default: <code>None</code>)   This is the path to file that contains all of the training configuration settings.   Please refer to the Experiment Configurations for more information.</p> </li> <li> <p><code>config_name</code>:   (Type: String,    Default: <code>None</code>)   This specifies the name of configuration setting to use from the file the file that   contains all of the training configuration settings specified above. Please refer to the Experiment Configurations for more information.</p> </li> <li> <p><code>subset</code>:   (Type: Positive Integer,    Default: <code>None</code>)   This specifies if there is a need to take a random subset of the dataset. This can be useful initially when building the model or debugging to remove the bugs. It will be much faster to train on a small subset of the data to detect bugs. Specify <code>None</code> to not create subset of the data.</p> </li> <li> <p><code>log_name</code>:   (Type: String,    Default: <code>None</code>)   This specifies the name of the project for logging metrics to the Weights and Biases account. More information about metrics and logging are mentioned in the following sections. Specify <code>None</code> to not log anything to Weights and Biases.</p> </li> <li> <p><code>num_classes</code>:   (Type: Positive Integer,    Default: <code>2</code>)   This specifies the number of classes in the dataset. For this particular example since there is only one object to be detected this parameter should be specified as 2. Note that 2 classes are specified instead of 1 because that is the PyTorch convention. One class represents the background and one class represents the object to detect i.e. Table.</p> </li> </ul>"},{"location":"guides/training/command/#example-of-the-training-command","title":"Example of the Training Command","text":"<p>An example of training command is as follows. It trains the RetinaNet model for 50 epochs, with only 10 images randomly selected from the dataset, 8 of which will be used as the training images and 2 as evaluation images. It also provides the path for the dataset and the path to save the checkpoint. </p> <pre><code>python network/trainers.py --dataset_path \"datasets/original/\" --epochs 50 --checkpoint_path \"checkpoint/\" --config_path \"experiment-configs.yml\" --config_name \"vanilla_fasterrcnn_v2_1\" --log_name \"test-test\"\n</code></pre>"},{"location":"guides/training/configs/","title":"Experiment Configurations","text":"<p>This section will give information about the usage of the experiment configurations that is required for training of the networks.</p>"},{"location":"guides/training/configs/#why-do-we-need-an-experiment-configuration-file","title":"Why do we need an experiment configuration file?","text":"<p>While developing a machine learning application, it can be very hard to keep track of all the parameters that we change trying to attain higher accuracy. This case is no different. There are a number of object detection models that can be used for detection of tables in  the PDF documents, each of these models will have their own set of hyperparameters that need to be tuned to achieve optimal performance.</p> <p>Keeping this in mind, an separate yaml file must be created (provided  with this repository) that includes the parameters, models and data modules that were used while training. This will also help other people in reproducing the work that was done previously. </p>"},{"location":"guides/training/configs/#breakdown-of-an-experiment-configuration","title":"Breakdown of an Experiment Configuration","text":"<p>One example of such configuration will be mentioned below, the rest can be developed easily by following the same pattern.</p> <pre><code>vanilla:\n  name: \"vanilla\"\n  model: \"VanillaRetinaNet\"\n  datamodule: \"TableDatasetModule\"\n  learning_rate: 1e-5\n  batch_size: 2\n  train_eval_split: 0.8\n</code></pre> <p>As it can be seen from the configuration file, the formatting is in YAML. The header of the configuration (in this case \"vanilla\") specifies the name of the configuration. This name will need to be specified in the training command when starting a trainig run. </p> <p>Following are the parameters that need to supplied in an experiment configuration:</p> <ul> <li> <p><code>name</code>: This is the name of the configuration, this name will also be appended as a tag in the Weights and Biases run. See the Metrics for more information.</p> </li> <li> <p><code>model</code>: This specifies the name of the model to use. The name of the model should exactly the same as name of the class that implements the model in the <code>models.py</code> file in the networks folder.</p> </li> <li> <p><code>datamodule</code>: This specifies the datamodule to use for the training run. As for the models, the name should match the name of the class that implements the required datamodule in the <code>datamodules.py</code> file in the networks folder.</p> </li> <li> <p><code>learning_rate</code>: This will specify the learning rate in the training run.</p> </li> <li> <p><code>batch_size</code>: This parameter specifies the number of images that will be fed to the model in each iteration during the training phase.</p> </li> <li> <p><code>train_eval_split</code>: This is a float number between the range of 0 - 1 that specifies the split of the dataset between training and evaluation set. The number specified is for the size of the training set. For example: 0.8 will mean 80% of the dataset to use as training set and 20% of the dataset to use as evaluation set.</p> </li> </ul> <p>All of these configurations will be printed on the console when the training run starts so that it can be validated if the correct values were read.</p>"},{"location":"guides/training/dataset/","title":"About the Dataset","text":"<p>The way the code has been setup, it requires the dataset folder to be in a very specific format. The format is mentioned below.</p>"},{"location":"guides/training/dataset/#folder-structure-of-dataset","title":"Folder Structure of Dataset","text":"<p>The folder structure should be the following.</p> <pre><code>- dataset\n    - images\n    - annotations.csv\n    - classes.csv\n</code></pre> <p>Important</p> <p>The name of the files and the folder should be exactly like mentioned here. This is because the names are hard coded so that only the path to root of the dataset folder needs to supplied when training. The headers shown in description below is just for information and should not be included in the CSV files themselves. </p> <ul> <li><code>images</code>: A folder that contains all the images.</li> <li><code>annotations.csv</code>: Annotations CSV file that contains the annotations in the following format:    <code>image_id,x1,y1,x2,y2,class_name</code>   Note that the <code>image_id</code> should just be name of the file in the images folder and not a path to the image. The dataset implementation in the code takes care of the path automatically. <code>x1,y1</code> are the coordinates for the top-left of the box. <code>x2,y2</code> are the coordinates for the bottom-right of the box. </li> <li><code>classes.csv</code>: Class list CSV file that contains the list of class in the following format:    <code>class_name,class_id</code>   The object classes should start from 1 and not 0, 0 is reserved for background classes, if the dataset contains examples for the background class then 0 class_id can be used.</li> </ul>"},{"location":"guides/training/dataset/#some-notes","title":"Some Notes","text":"<p>There are some particularities about the current implementation of dataset that need to be noted:</p> <ul> <li> <p>There is no separate annotation file for the validation and training set. All the annotations should be mentioned in the <code>annotations.csv</code> file. Instead the Evaluation and Training split should be specified as an argument for the <code>trainer.py</code> file (See the 'How to Train the Networks' section).</p> </li> <li> <p>Negative examples are not allowed. These are images where there is no object to be detected. If such images are present, the loss is drived to infinity. This is a known issue currently for the code.</p> </li> </ul>"},{"location":"guides/training/metrics/","title":"About Logging and Evaluation Metrics","text":"<p>Evaluation metrics are critical in order to evaluate the model performance. It is necessary to keep track if these metrics throughout the training process as well in order make decisions about hyperparameters for example.</p>"},{"location":"guides/training/metrics/#weights-and-biases-integration","title":"Weights and Biases Integration","text":"<p>In order to keep track of the model performance throughout the training process, there is Weights and Biases integration built into the code. Weights and biases is an MLOps platform that allows us to keep track of Machine Learning experiements.</p> <p>In order to log values to the Weights and Biases dashboard, a project name needs to be specified when running the training script, see the 'How to Train the Networks' section for more information. In addition to specifying the name of the projects, an account at Weights and Biases is needed. After logging in, the secret key will be available at https://wandb.ai/authorize. Enter this key when prompted while running the training script. </p>"},{"location":"guides/training/metrics/#logged-metrics","title":"Logged Metrics","text":"<p>Different metrics are logged during the training and the validation phases. These can be visualised on the Weights and Biases dashboard. </p>"},{"location":"guides/training/metrics/#during-training","title":"During training","text":"<p>During the training process, following quantities are logged.</p> <ul> <li>Classification Loss: This is loss that the network calculates when classifying the region of objects into classes. For our case there is only one class of interest i.e. Table hence this loss goes down fairly quickly during the training phase.</li> <li>Regression Loss: This represents the loss that is calculated when the network tries to draw the bounding boxes around the tables. For us this quantity is more representative of the network training success.</li> <li>Total Loss: This is just the sum of both the classification and regression loss and provides the overview of the network training performance.</li> <li>Mean Total Loss: This is just the total loss averaged over the entire epoch.</li> </ul>"},{"location":"guides/training/metrics/#during-evaluation","title":"During Evaluation","text":"<p>During the evaluation of the dataset, following quantities are logged.</p> <ul> <li>Averaged IoU: During the evaluation phase, the network is asked to make bounding box predictions over an image. Those box predictions are taken and compared with the ground truth bounding boxes. An overlap is calculated using a measure called IoU (Intersection of Union). </li> <li>Precision: During the evaluation phase, the network tries its best to make predictions of where the tables are. The predictions are the bounding boxes. Each of these bounding boxes come with their confidence scores. We select some of these boxes with a certain confidence score threshold. In addition to the confidence score, we also compute the overlap of these boxes with the ground truths and only select the boxes with a certain overlap threshold. So after both of these thresholds applied, we will have final set of detected tables. Precision value will tell us how many of these detected tables were actually tables. </li> <li>Recall: The process of calculating the recall metric is very similar to precision as described above. After applying both the confidence score and IoU thresholds, we get a set of predicted detections for the table. Recall tells us how many of the tables were actually detected by the network. </li> </ul>"},{"location":"tools/","title":"Overview","text":"<p>The documentation is structured in a way to separate the documentation of the command-line and other utilities from the guides that incorporate the usage of these utilities. </p> <p>This section of the documentation will provide information about all of the utilities that are available to the end user. Please check the navigation bar to the left in order to identify the utility that you are looking for or you can also search the relevant keywords for the utility that you need.</p>"},{"location":"tools/network/","title":"Overview","text":"<p>The network tools are the set of utilities that will provide the core utilities in order to extract the values of interest from the PDF files. There are multiple utilities available that can be used step-by-step on folder full of PDF files or single files. The major ones are discussed in this section.</p>"},{"location":"tools/network/evaluate/","title":"Evaluate","text":"<p>This is a simple utility that can be used to evaluate different approaches from the pipeline of table content extraction. The Evaluation Guide makes use of the functionality of this utility, the only difference is that it provides a much cleaner interface using the makefile. Same results can be achieved using this utility as well. </p>"},{"location":"tools/network/evaluate/#purpose","title":"Purpose","text":"<p>This utility provides an ability to reproduce the results generated during the work done as part of the master thesis. In essence, these results can be used as a baseline for future work to be done on the project.</p>"},{"location":"tools/network/evaluate/#how-it-works","title":"How it works","text":"<p>The utility provides the ability to replicate the experiments done during the master thesis for the evaluation of the complete pipeline. For that, some known data is required on which different parts of the pipeline can be executed in order to make predictions and compare them. This data is available in the Downloads section.</p>"},{"location":"tools/network/evaluate/#parameters","title":"Parameters","text":"<p>Following parameters are required to run the script:</p> <ul> <li> <p><code>-t</code> or <code>--type</code>: This parameter specifies the type of evaluation to perform. The options are 'detection' which denotes Table Detection Evaluation, 'classification' which denotes Table Classification Evaluation, 'complete' which denotes Complete Pipeline Evaluation and finally 'all' which will perform previous three evaluations together and show the results collectively.</p> </li> <li> <p><code>-p</code> or <code>--path</code>: This parameter specifies the path to the folder that contains the appropriate data according to the evaluation type specified.</p> </li> </ul>"},{"location":"tools/network/evaluate/#example","title":"Example","text":"<p>An example on how to run the script is following:</p> <pre><code>python network/evaluate.py -t complete -p evaluation-data/complete/\n</code></pre>"},{"location":"tools/network/extract-tables/","title":"Extract Tables","text":"<p>A utility to get the textual data of tables from the PDF files.</p>"},{"location":"tools/network/extract-tables/#purpose","title":"Purpose","text":"<p>After the tables have been detected by a machine learning model or the regions are indicated by a human. This utility can be used to extract the table content from the PDF files and put them into an excel file.</p>"},{"location":"tools/network/extract-tables/#how-it-works","title":"How it works","text":"<p>For this utility to work, two major things are required. One is a folder that contains all of the PDF files that need to be evaluated, the other one is the CSV file that contains the regions in the PDF file where the tables are located. </p> <p>The CSV file such ideally be generated by the trained Deep Learning model which can be accessed from the Infer Tool. It should generate the CSV in the following format.</p> <pre><code>filename,pageno,x1,y1,x2,y2\n601fa1f389734.pdf,1,258,2891,2155,5165\n601fa1f389734.pdf,1,244,5426,2107,6218\n601fa1f389733.pdf,2,210,3268,3162,4738\n601fa1f389733.pdf,2,2079,5861,3125,6346\n601fa1f389733.pdf,2,177,5774,2042,6376\n601fa1f389733.pdf,2,1899,4921,3177,5538\n601fa1f389733.pdf,2,216,4897,1865,5516\n601fa1f389733.pdf,2,220,1165,3135,2009\n601fa1f389733.pdf,2,246,2317,3114,2998\n</code></pre> <p>Here, the <code>filename</code> is the name of the PDF file that needs to evaluated. The filename should contain the extension i.e. '.pdf'. The <code>pageno</code> field specifies the page on which the table exists. The coordinates <code>x1</code>, <code>y1</code>, <code>x2</code> and <code>y2</code> are that of the table found. <code>x1</code> and <code>y1</code> specify the top left corner of the table and <code>x2</code> and <code>y2</code> specify the bottom corner of the table.</p> <p>Important</p> <p>The x1, y1, x2, y2 coordinates mentioned in the file should be in the PDF coordinate space. This means that the (0, 0) position of the x and y-axis respectively is at the bottom-left of the page. This is different than what normally is the case where the (0, 0) position is the top-left of the page. This is done because the table extraction utilities e.g. Camelot expect the table regions in PDF coordinate space.</p> <p>Note</p> <p>The header filename,pageno,x1,y1,x2,y2 are just added in the CSV representation here for information. The infer tool will not generate this header. This utility also expects the CSV file without this header, however the order of information should be respected.</p> <p>The utility will go through the PDF files one by one. Take in all the tables found in that file, extract the textual data in a tabular format and save them as a file. In case an excel file is required, it will generate an excel file for that PDF that will contain all of the tables. Each table will constitute one sheet in the excel document.</p>"},{"location":"tools/network/extract-tables/#parameters","title":"Parameters","text":"<p>Following parameters are required to run the script:</p> <ul> <li> <p><code>-f</code> or <code>--folder</code>: This is the path to the folder that contains all of the PDF files.</p> </li> <li> <p><code>-c</code> or <code>--csv</code>: This is the path to the CSV file that contains information about the table areas in the PDF files. See the section above for more information about this file.</p> </li> <li> <p><code>-r</code> or <code>--reader</code>: This parameter specifies the type of reader that will be used to read the tables from the PDF files provided their coordinate locations. Three options are available: 'baseline', 'camelot' and 'tabula'. Baseline is very simple custom rule-based table extractor that is implemented just to provide a comparison with the other utilities. Camelot and Tabula, on the other hand, are more sophisticated Python utilities for extracting the tables.</p> </li> <li> <p><code>-o</code> or <code>--output</code>: This is the path to the folder where the generated files will be stored.</p> </li> </ul>"},{"location":"tools/network/extract-tables/#example","title":"Example","text":"<p>An example on how to run the script is following:</p> <pre><code>python extract-tables.py -f ../misc/pdf-test/ -c ../misc/pdf-test/output6.csv -o ../misc/pdf-test/ -t excel\n</code></pre>"},{"location":"tools/network/infer/","title":"Infer Tool","text":"<p>A command-line tool for generating an output file i.e. CSV for all the bounding boxes found in all of the pdf files that are provided as input to this utility.</p>"},{"location":"tools/network/infer/#purpose","title":"Purpose","text":"<p>This tool was developed in order to get the inference results from the model and save them as a CSV file so that the data can be extracted from the tables that were detected.</p> <p>The tool can either take in a single pdf as an input or it can take folder that contains multiple pdf files as an input.</p>"},{"location":"tools/network/infer/#how-it-works","title":"How it works","text":"<p>The utility when provided with an appropriate model, the weights for the model and the an appropriate input pdf file, can detect the tables in the input pdf file and save the output as a CSV file.</p> <p>It takes the input file, converts it into an image in case of a pdf file, then passes that image through the specified model, gets the bounding boxes with confidence score higher than the threshold provided. It then collects all the coordinates of the detected tables into a list and saves it in a CSV file.</p> <p>An example of such a CSV file is shown below:</p> <p>Note</p> <p>The header \"filename,pageno,x1,y1,x2,y2\" shown below is just for reference and it will not appear in the actual file generated by this tool.</p> <pre><code>filename,pageno,x1,y1,x2,y2\n601fa1f389734.pdf,1,258,2891,2155,5165\n601fa1f389734.pdf,1,244,5426,2107,6218\n601fa1f389733.pdf,2,210,3268,3162,4738\n601fa1f389733.pdf,2,2079,5861,3125,6346\n601fa1f389733.pdf,2,177,5774,2042,6376\n601fa1f389733.pdf,2,1899,4921,3177,5538\n601fa1f389733.pdf,2,216,4897,1865,5516\n601fa1f389733.pdf,2,220,1165,3135,2009\n601fa1f389733.pdf,2,246,2317,3114,2998\n</code></pre> <p>The first column is the name of the pdf was that was provided, the second column contains the page number on which the table was found. The column 3 through 6 mention the coordinates of the table that was detected. 'x1' and 'y1' are the coorindates for the top-left corner and 'x2' and 'y2' shows the bottom-right coordinates of the bounding box for the table that was detected.</p> <p>Important</p> <p>The x1, y1, x2, y2 coordinates generated will be in the PDF coordinate space. This means that the (0, 0) position of the x and y-axis respectively is at the bottom-left of the page. This is different than what normally is the case where the (0, 0) position is the top-left of the page. This is done because the table extraction utilities e.g. Camelot expect the table regions in PDF coordinate space.</p> <p>If a page in PDF contains no table, according to the model, then it is not mentioned at all in the CSV file.</p> <p>Limitation</p> <p>At the moment, due to limitations of the table text extraction utility, the PDF file that do not contain any text layer e.g. if they are scanned, will be skipped. Bounding boxes of tables on such files cannot be extracted at the moment.</p>"},{"location":"tools/network/infer/#parameters","title":"Parameters","text":"<p>The following parameters can be specified to run the command line utility.</p>"},{"location":"tools/network/infer/#required-parameters","title":"Required Parameters","text":"<ul> <li> <p><code>-t</code> or <code>--type</code>: The type of file on which to perform the inference. Valid options are 'pdf', 'pdfs_folder', 'image' or 'images_folder'. To visualised a single pdf file 'pdf' should be specified. To visualise a folder of pdf file, 'pdfs_folder' should be specified. To visualise a single image, 'image' should be specified and lastly in order to visualise a folder of images, 'images_folder' should be specified. Please make sure to include the full path to the files, in case of individual image or pdf, when specifying the path parameter below.</p> </li> <li> <p><code>-p</code> or <code>--path</code>: This is the path to the type of file that was specified above. </p> </li> <li> <p><code>-m</code> or <code>--model</code>: This is the name of the model that should be used to generate the bounding boxes on the images provided. The name of the model should exactly match the name of the class that implements the model in the models.py file.</p> </li> <li> <p><code>-w</code> or <code>--weights</code>: In this parameter, the path to the Pytorch Lightning checkpoint should be provided that contains the trained weights for the model that was specified above. Please make sure that the checkpoint provided belongs to the model that was specified above. You can find the checkpoints in the Downloads section.</p> </li> <li> <p><code>-o</code> or <code>--output</code>: This parameter will specify the path, where to save the CSV file. Please specify the name of the CSV file alongwith the extension '.csv' when specifying the folder in which to save the CSV file.</p> </li> </ul>"},{"location":"tools/network/infer/#optional-parameters","title":"Optional Parameters","text":"<ul> <li> <p><code>-h</code> or <code>--help</code>: This will show instructions on how to use the utility.</p> </li> <li> <p><code>-c</code> or <code>--confidence</code>: This parameter specifies the confidence threshold. It is a floating point number between 0 and 1. When the model makes predictions, it specfies how confident it is on those predicitons as well. This parameter will specify the cutoff, the predictions below this cutoff will not be considered. This is an optional parameter, if no value is specified then 0.75 will be taken.</p> </li> <li> <p><code>-d</code> or <code>--dpi</code>: In order to detect tables in the PDF file. Each individual page is first converted to an image that will then be passed through the model. This parameter specifies the DPI for that rendered image. The higher this number, the higher the resolution of the rendered image will be but the process of inference will also will be slower. This parameter also needs to be selected in accordance with the utility that will extract the textual data from the table e.g. Camelot. This is because if the resolution is different, the coordinates of the detected table will also be different.</p> </li> </ul>"},{"location":"tools/network/infer/#example","title":"Example","text":"<p>An example of how to run the utility is provided below:</p> <pre><code>python infer.py -t pdf -m VanillaRetinaNet -w ../misc/best-chkpnt-epoch=35.ckpt -c 0.80 -p ../misc/601fa1f389733.pdf -o ../misc/output.csv\n</code></pre>"},{"location":"tools/network/train/","title":"Train Tool","text":"<p>This utility has not been implemented as of yet. The functionality for training the models is still only available in the trainers.py file in the network directory. Please take a look at this guide here for more information: Training the Networks.</p>"},{"location":"tools/network/visualise/","title":"Visualise Tool","text":"<p>A command-line tool for visualing the outputs of the model given the trained checkpoint and an input image or a pdf file.</p>"},{"location":"tools/network/visualise/#purpose","title":"Purpose","text":"<p>This tool was developed in order to get a lens into what the network is doing before running the model in production and saving all of the results in a file.</p> <p>The purpose of this utility is to generate bounding boxes on a variety of input file types. This can be very useful for debugging purposes to see how well the network is detecting the tables.</p> <p>The goal was to create tool that can take a diverse range of input files, to that effect, this utility can visualise images, all of the images in a folder, pdf files and all of the pdf files in any given folder.</p>"},{"location":"tools/network/visualise/#how-it-works","title":"How it works","text":"<p>The utility when provided with an appropriate model, the weights for the model and the an appropriate input file, can show the user what the model is detecting by drawing bounding boxes on the detected tables.</p> <p>It takes the input file, converts it into an image in case of a pdf file, then passes that image through the specified model, gets the bounding boxes with confidence score higher than the threshold provided. It then draws the selected bounding boxes on the image and shows it on the screen.</p> <p>Each image will be shown one-by-one. Enter key needs to be pressed to go to the next image.</p> <p>Limitation</p> <p>At the moment, due to limitations of the table text extraction utility, the PDF file that do not contain any text layer e.g. if they are scanned, will be skipped. Such files cannot be visualised with this tool.</p>"},{"location":"tools/network/visualise/#parameters","title":"Parameters","text":"<p>The following parameters can be specified to run the command line utility.</p>"},{"location":"tools/network/visualise/#required-parameters","title":"Required Parameters","text":"<ul> <li> <p><code>-t</code> or <code>--type</code>: The type of file on which to perform the inference. Valid options are 'pdf', 'pdfs_folder', 'image' or 'images_folder'. To visualised a single pdf file 'pdf' should be specified. To visualise a folder of pdf file, 'pdfs_folder' should be specified. To visualise a single image, 'image' should be specified and lastly in order to visualise a folder of images, 'images_folder' should be specified. Please make sure to include the full path to the files, in case of individual image or pdf, when specifying the path parameter below.</p> </li> <li> <p><code>-p</code> or <code>--path</code>: This is the path to the type of file that was specified above. </p> </li> <li> <p><code>-m</code> or <code>--model</code>: This is the name of the model that should be used to generate the bounding boxes on the images provided. The name of the model should exactly match the name of the class that implements the model in the models.py file.</p> </li> <li> <p><code>-w</code> or <code>--weights</code>: In this parameter, the path to the Pytorch Lightning checkpoint should be provided that contains the trained weights for the model that was specified above. Please make sure that the checkpoint provided belongs to the model that was specified above.You can find the checkpoints in the Downloads section.</p> </li> </ul>"},{"location":"tools/network/visualise/#optional-parameters","title":"Optional Parameters","text":"<ul> <li> <p><code>-h</code> or <code>--help</code>: This will show instructions on how to use the utility.</p> </li> <li> <p><code>-c</code> or <code>--confidence</code>: This parameter specifies the confidence threshold. It is a floating point number between 0 and 1. When the model makes predictions, it specfies how confident it is on those predicitons as well. This parameter will specify the cutoff, the predictions below this cutoff will not be considered. This is an optional parameter, if no value is specified then 0.75 will be taken.</p> </li> <li> <p><code>-r</code> or <code>--randomise</code>: In case a folder of PDF needs to be visualised, if this parameter is specified as 'True' then the files to be visualised will be picked up randomly. The default value is False. This parameter will not affect other visualisation cases.</p> </li> </ul>"},{"location":"tools/network/visualise/#example","title":"Example","text":"<p>An example of how to run the utility is provided below:</p> <pre><code>python visualise.py -t pdf -m VanillaRetinaNet -w ../misc/best-chkpnt-epoch=35.ckpt -c 0.80 -p ../misc/601fa1f389733.pdf\n</code></pre>"},{"location":"tools/utils/","title":"Utilities","text":"<p>This section will provide an overview about the smaller scripts that were developed to facilite the process of training, testing and deploying the machine learning models.</p>"},{"location":"tools/utils/data-house-keeper/","title":"Data House Keeper","text":"<p>This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images.</p> <p>One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed.</p> <p>In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts.</p>"},{"location":"tools/utils/data-house-keeper/#purpose","title":"Purpose","text":"<p>Data House Keeper helps in deleting extra files from a PascalVOC folder i.e. Extra XML and JPG files. It also can rename the files from the number specified so that all the XML and JPG files can be organised.</p> <p>Limitation</p> <p>The image files can only be in the JPG format.</p> <p>Limitation</p> <p>The name of the both counterparts (the image and its annotation XML) should have the same name for this script to work. This is the case for the files produced by labelimg, so if you're using that tool, it should be fine.</p>"},{"location":"tools/utils/data-house-keeper/#how-it-works","title":"How it works","text":"<ul> <li> <p>Once the script is started, it will go through the folder and find all the JPGs that have their XML files already present, it will match the names for doing so.</p> </li> <li> <p>If there are extra files present i.e. JPG files that have no matching XML files and vice versa, it will then ask if you want to delete the extra files, if the extra files are deleted only then it will proceed to the next step otherwise quit.</p> </li> <li> <p>If the extra files have been cleaned up (or there are no extra files) the script will ask if you want to rename the files, if you say yes then it will ask a number from which to start the numbering, this can be useful if you already have images in another folder and you just want to add more labelled data.</p> </li> </ul>"},{"location":"tools/utils/data-house-keeper/#parameters","title":"Parameters","text":"<p>Following parameters are required to run the script:</p> <ul> <li><code>pascalvoc_path</code>: This is the path to folder that contains the PascalVOC format annotations.</li> </ul> <p>Note</p> <p>The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above.</p>"},{"location":"tools/utils/data-house-keeper/#example","title":"Example","text":"<p>An example on how to run the script is following:</p> <pre><code>python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"\n</code></pre>"},{"location":"tools/utils/pascalvoc-to-csv/","title":"PascalVOC to CSV","text":"<p>This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images.</p> <p>One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed.</p> <p>In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts.</p>"},{"location":"tools/utils/pascalvoc-to-csv/#purpose","title":"Purpose","text":"<p>This script converts the image annotations/labellings from the PascalVOC (XML) format to CSV and writes them as a CSV file.</p> <p>Warning</p> <p>The script only reads the XML annotation files and converts them to a single CSV file for saving. It is essential that before running this script, that the folder has been cleaned up using the Data House Keeper script. </p>"},{"location":"tools/utils/pascalvoc-to-csv/#how-it-works","title":"How it works","text":"<ul> <li>It will go through each XML file in the folder and get the respective labelled objects. </li> </ul> <p>Warning</p> <p>Since this script was purpose built for the table detection algorithm, it will set the labels of all the annotations to the same thing i.e. \"Table\". This script is not made for the case of multiple objects.</p> <ul> <li> <p>It will assemble one big Pandas DataFrame that will contain the annotations from all of the XML files present.</p> </li> <li> <p>Finally, it will ask if you want to save the CSV file, if you say yes then it will ask for the name of the file. Please enter a valid name i.e. not empty to save the file. Do not enter the extension i.e. \".csv\" in the filename, that is understood.</p> </li> </ul>"},{"location":"tools/utils/pascalvoc-to-csv/#parameters","title":"Parameters","text":"<p>Following parameters are required to run the script:</p> <ul> <li><code>pascalvoc_path</code>: This is the path to folder that contains the PascalVOC format annotations.</li> </ul> <p>Note</p> <p>The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above.</p>"},{"location":"tools/utils/pascalvoc-to-csv/#example","title":"Example","text":"<p>An example on how to run the script is following:</p> <pre><code>python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"\n</code></pre>"},{"location":"tools/utils/pdf-dedup/","title":"Data House Keeper","text":"<p>This script was created as part of one of the pre-processing steps before the PDFs are passed through the network for inference and detection of tables.</p>"},{"location":"tools/utils/pdf-dedup/#purpose","title":"Purpose","text":"<p>When the PDF files were collected during the data scraping process, a lot of duplicates were created because same files were saved under conflicting namees provided by the website. This utility helps in detecting those duplicates. </p>"},{"location":"tools/utils/pdf-dedup/#how-it-works","title":"How it works","text":"<p>When the folder of PDF was analysed, it was found that the duplicate files had the identical file size. Initially the goal was to detect the duplicates only on the basis of the file sizes.</p> <p>This turns out was not enough to detect the duplicates, since the files from the same manufacturers had the same file sizes even if the content of the PDF files was different. To counter this, the last-modified time of the PDF file was also considered. PDFs that were duplicates had the last-modified time very close to each other i.e. within 5 minutes.</p> <p>To take that into account, the metadata of the files is read. The last-modified time is rounded off to the closest 5 minute mark. Then duplicates are detected based on the file sizes and the rounded-off last-modified time.</p>"},{"location":"tools/utils/pdf-dedup/#parameters","title":"Parameters","text":"<p>The following parameters can be specified to run the script:</p>"},{"location":"tools/utils/pdf-dedup/#required-parameters","title":"Required Parameters","text":"<ul> <li> <p><code>-p</code> or <code>--path</code>: This is the path to the folder that contains the PDF files.</p> </li> <li> <p><code>-a</code> or <code>--action</code>: Once the PDF files have been detected as duplicates, this parameter will specify the action that will be taken against them. The valid actions are 'noaction', 'delete' and 'move'. If 'noaction' is specified the no action will be taken against the duplicated files. If 'delete' is specified then the duplicated files will be deleted. In case 'move' is specified then a new folder named 'Duplicates' will be created and the files will be moved over to that folder. Please make sure that a folder named 'Duplicates' does not already exist.</p> </li> </ul>"},{"location":"tools/utils/pdf-dedup/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>-h</code> or <code>--help</code>: This will show instructions on how to use the utility.</li> </ul>"},{"location":"tools/utils/pdf-dedup/#example","title":"Example","text":"<p>An example on how to run the script is following:</p> <pre><code>python pdf-dedup.py -p ../pdf-files/ -a noaction\n</code></pre>"}]}