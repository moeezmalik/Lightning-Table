{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Detecting Tables using Pytorch and Lightning This repository contains the code for detecting tables in PDF documents by using the PyTorch and the Lightning framework. Warning This repository is still a work in progress and things might change drastically. Requirements The requirements for this repository can be found in the requirements.txt file in the root of respository. The requirements can be installed using the following command using pip: pip install -r requirements.txt Folder Structure of the Repository The repository folders are structured in the following way. - network - datamodules.py - models.py - trainers.py - transforms.py - utilities.py - utils - visualisers.py - requirements.txt The network folder contains all the files that deal for the creation, configuration, training and inference of PyTorch based models. The datamodules.py file contains everything related to the management of dataset for the training of the models. The models.py file contains all the PyTorch based models e.g. the RetinaNet. The trainers.py contains all the necessary assembly of functions to initialise the training of the models. More information about the training of the models is provided below. Since we are dealing with object detection models, the runtime transforms that might be required to apply to the images need to be applied to the annotations i.e. bounding boxes as well. These special transforms are present in the transforms.py file. The utilities.py file contains helper utilities for the setup of the models. The utils folder provides some utilities that might be needed to interface with the network. visualisers.py provides some functions for the purposes of visualising the detections made by the network. requirements.txt file contains the required Python packages to run the code in this repository.","title":"Home"},{"location":"#detecting-tables-using-pytorch-and-lightning","text":"This repository contains the code for detecting tables in PDF documents by using the PyTorch and the Lightning framework. Warning This repository is still a work in progress and things might change drastically.","title":"Detecting Tables using Pytorch and Lightning"},{"location":"#requirements","text":"The requirements for this repository can be found in the requirements.txt file in the root of respository. The requirements can be installed using the following command using pip: pip install -r requirements.txt","title":"Requirements"},{"location":"#folder-structure-of-the-repository","text":"The repository folders are structured in the following way. - network - datamodules.py - models.py - trainers.py - transforms.py - utilities.py - utils - visualisers.py - requirements.txt The network folder contains all the files that deal for the creation, configuration, training and inference of PyTorch based models. The datamodules.py file contains everything related to the management of dataset for the training of the models. The models.py file contains all the PyTorch based models e.g. the RetinaNet. The trainers.py contains all the necessary assembly of functions to initialise the training of the models. More information about the training of the models is provided below. Since we are dealing with object detection models, the runtime transforms that might be required to apply to the images need to be applied to the annotations i.e. bounding boxes as well. These special transforms are present in the transforms.py file. The utilities.py file contains helper utilities for the setup of the models. The utils folder provides some utilities that might be needed to interface with the network. visualisers.py provides some functions for the purposes of visualising the detections made by the network. requirements.txt file contains the required Python packages to run the code in this repository.","title":"Folder Structure of the Repository"},{"location":"guides/training/","text":"How to Train the Networks The repository will contain the code to train multiple networks on the dataset. The trainers.py file can be used for the purposes of training the available models. Currently only the default PyTorch RetinaNet is available for the training purposes. Known Issues Following are some known issues with the code: Negative examples do not work while training. They increase the loss to infinity from which the network never recovers. Tested On The code has been tested to run on: macOS 12.4 Monterey on Intel Macbook Pro 2019 Google Colab","title":"Overview"},{"location":"guides/training/#how-to-train-the-networks","text":"The repository will contain the code to train multiple networks on the dataset. The trainers.py file can be used for the purposes of training the available models. Currently only the default PyTorch RetinaNet is available for the training purposes.","title":"How to Train the Networks"},{"location":"guides/training/#known-issues","text":"Following are some known issues with the code: Negative examples do not work while training. They increase the loss to infinity from which the network never recovers.","title":"Known Issues"},{"location":"guides/training/#tested-on","text":"The code has been tested to run on: macOS 12.4 Monterey on Intel Macbook Pro 2019 Google Colab","title":"Tested On"},{"location":"guides/training/checkpoints/","text":"About the Checkpoints The checkpoint is a terminology taken from the PyTorch Lightning framework. It is a file that contains the state of the model. Checkpoints are saved successively throughout the training process. The benefit of keeping checkpoints is that we can keep track and save the model state that performed the best since training of the network is an iterative process. Following checkpoints are saved through the training process: Checkpoint for the last epoch Checkpoint for the best Evaluation Average IoU Checkpoint for the best Evalutaion Precision (at 75% Confidence Score and 90% IoU) Checkpoint for the best Evaluation Recall (at 75% Confidence Score and 90% IoU)","title":"Checkpoints"},{"location":"guides/training/checkpoints/#about-the-checkpoints","text":"The checkpoint is a terminology taken from the PyTorch Lightning framework. It is a file that contains the state of the model. Checkpoints are saved successively throughout the training process. The benefit of keeping checkpoints is that we can keep track and save the model state that performed the best since training of the network is an iterative process. Following checkpoints are saved through the training process: Checkpoint for the last epoch Checkpoint for the best Evaluation Average IoU Checkpoint for the best Evalutaion Precision (at 75% Confidence Score and 90% IoU) Checkpoint for the best Evaluation Recall (at 75% Confidence Score and 90% IoU)","title":"About the Checkpoints"},{"location":"guides/training/command/","text":"Training Command This section will show how to initiate the training and what parameters are offered in the training command. Breakdown of the Training Command The trainer.py file accepts the following arguments when initialising the training: dataset_path : ( Type : String, Default : ./ ) This specifies the path to the dataset. More information about the format of the dataset folder is mentioned in the following sections. checkpoint_path : ( Type : String, Default : ./ ) This is used to specify the path where the checkpoints should be saved. More information about this is mentioned in the following sections. epochs : ( Type : Postive Integer, Default : 50 ) This specifies the number of epochs to train for. config_path : ( Type : String, Default : None ) This is the path to file that contains all of the training configuration settings. Please refer to the Experiment Configurations for more information. config_name : ( Type : String, Default : None ) This specifies the name of configuration setting to use from the file the file that contains all of the training configuration settings specified above. Please refer to the Experiment Configurations for more information. subset : ( Type : Positive Integer, Default : None ) This specifies if there is a need to take a random subset of the dataset. This can be useful initially when building the model or debugging to remove the bugs. It will be much faster to train on a small subset of the data to detect bugs. Specify None to not create subset of the data. log_name : ( Type : String, Default : None ) This specifies the name of the project for logging metrics to the Weights and Biases account. More information about metrics and logging are mentioned in the following sections. Specify None to not log anything to Weights and Biases. num_classes : ( Type : Positive Integer, Default : 2 ) This specifies the number of classes in the dataset. For this particular example since there is only one object to be detected this parameter should be specified as 2. Note that 2 classes are specified instead of 1 because that is the PyTorch convention. One class represents the background and one class represents the object to detect i.e. Table. Example of the Training Command An example of training command is as follows. It trains the RetinaNet model for 50 epochs, with only 10 images randomly selected from the dataset, 8 of which will be used as the training images and 2 as evaluation images. It also provides the path for the dataset and the path to save the checkpoint. python network/trainers.py --dataset_path \"datasets/original/\" --epochs 50 --checkpoint_path \"checkpoint/\" --config_path \"experiment-configs.yml\" --config_name \"vanilla_fasterrcnn_v2_1\" --log_name \"test-test\"","title":"Training Command"},{"location":"guides/training/command/#training-command","text":"This section will show how to initiate the training and what parameters are offered in the training command.","title":"Training Command"},{"location":"guides/training/command/#breakdown-of-the-training-command","text":"The trainer.py file accepts the following arguments when initialising the training: dataset_path : ( Type : String, Default : ./ ) This specifies the path to the dataset. More information about the format of the dataset folder is mentioned in the following sections. checkpoint_path : ( Type : String, Default : ./ ) This is used to specify the path where the checkpoints should be saved. More information about this is mentioned in the following sections. epochs : ( Type : Postive Integer, Default : 50 ) This specifies the number of epochs to train for. config_path : ( Type : String, Default : None ) This is the path to file that contains all of the training configuration settings. Please refer to the Experiment Configurations for more information. config_name : ( Type : String, Default : None ) This specifies the name of configuration setting to use from the file the file that contains all of the training configuration settings specified above. Please refer to the Experiment Configurations for more information. subset : ( Type : Positive Integer, Default : None ) This specifies if there is a need to take a random subset of the dataset. This can be useful initially when building the model or debugging to remove the bugs. It will be much faster to train on a small subset of the data to detect bugs. Specify None to not create subset of the data. log_name : ( Type : String, Default : None ) This specifies the name of the project for logging metrics to the Weights and Biases account. More information about metrics and logging are mentioned in the following sections. Specify None to not log anything to Weights and Biases. num_classes : ( Type : Positive Integer, Default : 2 ) This specifies the number of classes in the dataset. For this particular example since there is only one object to be detected this parameter should be specified as 2. Note that 2 classes are specified instead of 1 because that is the PyTorch convention. One class represents the background and one class represents the object to detect i.e. Table.","title":"Breakdown of the Training Command"},{"location":"guides/training/command/#example-of-the-training-command","text":"An example of training command is as follows. It trains the RetinaNet model for 50 epochs, with only 10 images randomly selected from the dataset, 8 of which will be used as the training images and 2 as evaluation images. It also provides the path for the dataset and the path to save the checkpoint. python network/trainers.py --dataset_path \"datasets/original/\" --epochs 50 --checkpoint_path \"checkpoint/\" --config_path \"experiment-configs.yml\" --config_name \"vanilla_fasterrcnn_v2_1\" --log_name \"test-test\"","title":"Example of the Training Command"},{"location":"guides/training/configs/","text":"Experiment Configurations This section will give information about the usage of the experiment configurations that is required for training of the networks. Why do we need an experiment configuration file? While developing a machine learning application, it can be very hard to keep track of all the parameters that we change trying to attain higher accuracy. This case is no different. There are a number of object detection models that can be used for detection of tables in the PDF documents, each of these models will have their own set of hyperparameters that need to be tuned to achieve optimal performance. Keeping this in mind, an separate yaml file must be created (provided with this repository) that includes the parameters, models and data modules that were used while training. This will also help other people in reproducing the work that was done previously. Breakdown of an Experiment Configuration One example of such configuration will be mentioned below, the rest can be developed easily by following the same pattern. vanilla: name: \"vanilla\" model: \"VanillaRetinaNet\" datamodule: \"TableDatasetModule\" learning_rate: 1e-5 batch_size: 2 train_eval_split: 0.8 As it can be seen from the configuration file, the formatting is in YAML. The header of the configuration (in this case \"vanilla\") specifies the name of the configuration. This name will need to be specified in the trainig command when starting a trainig run. Following are the parameters that need to supplied in an experiment configuration: name : This is the name of the configuration, this name will also be appended as a tag in the Weights and Biases run. See the Metrics for more information. model : This specifies the name of the model to use. The name of the model should exactly the same as name of the class that implements the model in the models.py file in the networks folder. datamodule : This specifies the datamodule to use for the training run. As for the models, the name should match the name of the class that implements the required datamodule in the datamodules.py file in the networks folder. learning_rate : This will specify the learning rate in the training run. batch_size : This parameter specifies the number of images that will be fed to the model in each iteration during the training phase. train_eval_split : This is a float number between the range of 0 - 1 that specifies the split of the dataset between training and evaluation set. The number specified is for the size of the training set. For example: 0.8 will mean 80% of the dataset to use as training set and 20% of the dataset to use as evaluation set. All of these configurations will be printed on the console when the training run starts so that it can be validated if the correct values were read.","title":"Experiment Configurations"},{"location":"guides/training/configs/#experiment-configurations","text":"This section will give information about the usage of the experiment configurations that is required for training of the networks.","title":"Experiment Configurations"},{"location":"guides/training/configs/#why-do-we-need-an-experiment-configuration-file","text":"While developing a machine learning application, it can be very hard to keep track of all the parameters that we change trying to attain higher accuracy. This case is no different. There are a number of object detection models that can be used for detection of tables in the PDF documents, each of these models will have their own set of hyperparameters that need to be tuned to achieve optimal performance. Keeping this in mind, an separate yaml file must be created (provided with this repository) that includes the parameters, models and data modules that were used while training. This will also help other people in reproducing the work that was done previously.","title":"Why do we need an experiment configuration file?"},{"location":"guides/training/configs/#breakdown-of-an-experiment-configuration","text":"One example of such configuration will be mentioned below, the rest can be developed easily by following the same pattern. vanilla: name: \"vanilla\" model: \"VanillaRetinaNet\" datamodule: \"TableDatasetModule\" learning_rate: 1e-5 batch_size: 2 train_eval_split: 0.8 As it can be seen from the configuration file, the formatting is in YAML. The header of the configuration (in this case \"vanilla\") specifies the name of the configuration. This name will need to be specified in the trainig command when starting a trainig run. Following are the parameters that need to supplied in an experiment configuration: name : This is the name of the configuration, this name will also be appended as a tag in the Weights and Biases run. See the Metrics for more information. model : This specifies the name of the model to use. The name of the model should exactly the same as name of the class that implements the model in the models.py file in the networks folder. datamodule : This specifies the datamodule to use for the training run. As for the models, the name should match the name of the class that implements the required datamodule in the datamodules.py file in the networks folder. learning_rate : This will specify the learning rate in the training run. batch_size : This parameter specifies the number of images that will be fed to the model in each iteration during the training phase. train_eval_split : This is a float number between the range of 0 - 1 that specifies the split of the dataset between training and evaluation set. The number specified is for the size of the training set. For example: 0.8 will mean 80% of the dataset to use as training set and 20% of the dataset to use as evaluation set. All of these configurations will be printed on the console when the training run starts so that it can be validated if the correct values were read.","title":"Breakdown of an Experiment Configuration"},{"location":"guides/training/dataset/","text":"About the Dataset The way the code has been setup, it requires the dataset folder to be in a very specific format. The format is mentioned below. Folder Structure of Dataset The folder structure should be the following. - dataset - images - annotations.csv - classes.csv Important The name of the files and the folder should be exactly like mentioned here. This is because the names are hard coded so that only the path to root of the dataset folder needs to supplied when training. The headers shown in description below is just for information and should not be included in the CSV files themselves. images : A folder that contains all the images. annotations.csv : Annotations CSV file that contains the annotations in the following format: image_id,x1,y1,x2,y2,class_name Note that the image_id should just be name of the file in the images folder and not a path to the image. The dataset implementation in the code takes care of the path automatically. x1,y1 are the coordinates for the top-left of the box. x2,y2 are the coordinates for the bottom-right of the box. classes.csv : Class list CSV file that contains the list of class in the following format: class_name,class_id The object classes should start from 1 and not 0, 0 is reserved for background classes, if the dataset contains examples for the background class then 0 class_id can be used. Some Notes There are some particularities about the current implementation of dataset that need to be noted: There is no separate annotation file for the validation and training set. All the annotations should be mentioned in the annotations.csv file. Instead the Evaluation and Training split should be specified as an argument for the trainer.py file (See the 'How to Train the Networks' section). Negative examples are not allowed. These are images where there is no object to be detected. If such images are present, the loss is drived to infinity. This is a known issue currently for the code.","title":"About Datasets"},{"location":"guides/training/dataset/#about-the-dataset","text":"The way the code has been setup, it requires the dataset folder to be in a very specific format. The format is mentioned below.","title":"About the Dataset"},{"location":"guides/training/dataset/#folder-structure-of-dataset","text":"The folder structure should be the following. - dataset - images - annotations.csv - classes.csv Important The name of the files and the folder should be exactly like mentioned here. This is because the names are hard coded so that only the path to root of the dataset folder needs to supplied when training. The headers shown in description below is just for information and should not be included in the CSV files themselves. images : A folder that contains all the images. annotations.csv : Annotations CSV file that contains the annotations in the following format: image_id,x1,y1,x2,y2,class_name Note that the image_id should just be name of the file in the images folder and not a path to the image. The dataset implementation in the code takes care of the path automatically. x1,y1 are the coordinates for the top-left of the box. x2,y2 are the coordinates for the bottom-right of the box. classes.csv : Class list CSV file that contains the list of class in the following format: class_name,class_id The object classes should start from 1 and not 0, 0 is reserved for background classes, if the dataset contains examples for the background class then 0 class_id can be used.","title":"Folder Structure of Dataset"},{"location":"guides/training/dataset/#some-notes","text":"There are some particularities about the current implementation of dataset that need to be noted: There is no separate annotation file for the validation and training set. All the annotations should be mentioned in the annotations.csv file. Instead the Evaluation and Training split should be specified as an argument for the trainer.py file (See the 'How to Train the Networks' section). Negative examples are not allowed. These are images where there is no object to be detected. If such images are present, the loss is drived to infinity. This is a known issue currently for the code.","title":"Some Notes"},{"location":"guides/training/metrics/","text":"About Logging and Evaluation Metrics Evaluation metrics are critical in order to evaluate the model performance. It is necessary to keep track if these metrics throughout the training process as well in order make decisions about hyperparameters for example. Weights and Biases Integration In order to keep track of the model performance throughout the training process, there is Weights and Biases integration built into the code. Weights and biases is an MLOps platform that allows us to keep track of Machine Learning experiements. In order to log values to the Weights and Biases dashboard, a project name needs to be specified when running the training script, see the 'How to Train the Networks' section for more information. In addition to specifying the name of the projects, an account at Weights and Biases is needed. After logging in, the secret key will be available at https://wandb.ai/authorize . Enter this key when prompted while running the training script. Logged Metrics Different metrics are logged during the training and the validation phases. These can be visualised on the Weights and Biases dashboard. During training During the training process, following quantities are logged. Classification Loss: This is loss that the network calculates when classifying the region of objects into classes. For our case there is only one class of interest i.e. Table hence this loss goes down fairly quickly during the training phase. Regression Loss: This represents the loss that is calculated when the network tries to draw the bounding boxes around the tables. For us this quantity is more representative of the network training success. Total Loss: This is just the sum of both the classification and regression loss and provides the overview of the network training performance. Mean Total Loss: This is just the total loss averaged over the entire epoch. During Evaluation During the evaluation of the dataset, following quantities are logged. Averaged IoU: During the evaluation phase, the network is asked to make bounding box predictions over an image. Those box predictions are taken and compared with the ground truth bounding boxes. An overlap is calculated using a measure called IoU (Intersection of Union). Precision: During the evaluation phase, the network tries its best to make predictions of where the tables are. The predictions are the bounding boxes. Each of these bounding boxes come with their confidence scores. We select some of these boxes with a certain confidence score threshold. In addition to the confidence score, we also compute the overlap of these boxes with the ground truths and only select the boxes with a certain overlap threshold. So after both of these thresholds applied, we will have final set of detected tables. Precision value will tell us how many of these detected tables were actually tables. Recall: The process of calculating the recall metric is very similar to precision as described above. After applying both the confidence score and IoU thresholds, we get a set of predicted detections for the table. Recall tells us how many of the tables were actually detected by the network.","title":"Metrics"},{"location":"guides/training/metrics/#about-logging-and-evaluation-metrics","text":"Evaluation metrics are critical in order to evaluate the model performance. It is necessary to keep track if these metrics throughout the training process as well in order make decisions about hyperparameters for example.","title":"About Logging and Evaluation Metrics"},{"location":"guides/training/metrics/#weights-and-biases-integration","text":"In order to keep track of the model performance throughout the training process, there is Weights and Biases integration built into the code. Weights and biases is an MLOps platform that allows us to keep track of Machine Learning experiements. In order to log values to the Weights and Biases dashboard, a project name needs to be specified when running the training script, see the 'How to Train the Networks' section for more information. In addition to specifying the name of the projects, an account at Weights and Biases is needed. After logging in, the secret key will be available at https://wandb.ai/authorize . Enter this key when prompted while running the training script.","title":"Weights and Biases Integration"},{"location":"guides/training/metrics/#logged-metrics","text":"Different metrics are logged during the training and the validation phases. These can be visualised on the Weights and Biases dashboard.","title":"Logged Metrics"},{"location":"guides/training/metrics/#during-training","text":"During the training process, following quantities are logged. Classification Loss: This is loss that the network calculates when classifying the region of objects into classes. For our case there is only one class of interest i.e. Table hence this loss goes down fairly quickly during the training phase. Regression Loss: This represents the loss that is calculated when the network tries to draw the bounding boxes around the tables. For us this quantity is more representative of the network training success. Total Loss: This is just the sum of both the classification and regression loss and provides the overview of the network training performance. Mean Total Loss: This is just the total loss averaged over the entire epoch.","title":"During training"},{"location":"guides/training/metrics/#during-evaluation","text":"During the evaluation of the dataset, following quantities are logged. Averaged IoU: During the evaluation phase, the network is asked to make bounding box predictions over an image. Those box predictions are taken and compared with the ground truth bounding boxes. An overlap is calculated using a measure called IoU (Intersection of Union). Precision: During the evaluation phase, the network tries its best to make predictions of where the tables are. The predictions are the bounding boxes. Each of these bounding boxes come with their confidence scores. We select some of these boxes with a certain confidence score threshold. In addition to the confidence score, we also compute the overlap of these boxes with the ground truths and only select the boxes with a certain overlap threshold. So after both of these thresholds applied, we will have final set of detected tables. Precision value will tell us how many of these detected tables were actually tables. Recall: The process of calculating the recall metric is very similar to precision as described above. After applying both the confidence score and IoU thresholds, we get a set of predicted detections for the table. Recall tells us how many of the tables were actually detected by the network.","title":"During Evaluation"},{"location":"guides/utils/","text":"Utilities This section will provide an overview about the smaller scripts that were developed to facilite the process of training, testing and deploying the machine learning models.","title":"Overview"},{"location":"guides/utils/#utilities","text":"This section will provide an overview about the smaller scripts that were developed to facilite the process of training, testing and deploying the machine learning models.","title":"Utilities"},{"location":"guides/utils/data-house-keeper/","text":"Data House Keeper This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts. Purpose Data House Keeper helps in deleting extra files from a PascalVOC folder i.e. Extra XML and JPG files. It also can rename the files from the number specified so that all the XML and JPG files can be organised. Limitation The image files can only be in the JPG format. Limitation The name of the both counterparts (the image and its annotation XML) should have the same name for this script to work. This is the case for the files produced by labelimg, so if you're using that tool, it should be fine. How it works Once the script is started, it will go through the folder and find all the JPGs that have their XML files already present, it will match the names for doing so. If there are extra files present i.e. JPG files that have no matching XML files and vice versa, it will then ask if you want to delete the extra files, if the extra files are deleted only then it will proceed to the next step otherwise quit. If the extra files have been cleaned up (or there are no extra files) the script will ask if you want to rename the files, if you say yes then it will ask a number from which to start the numbering, this can be useful if you already have images in another folder and you just want to add more labelled data. Required Parameters Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above. Example An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"Data House Keeper"},{"location":"guides/utils/data-house-keeper/#data-house-keeper","text":"This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts.","title":"Data House Keeper"},{"location":"guides/utils/data-house-keeper/#purpose","text":"Data House Keeper helps in deleting extra files from a PascalVOC folder i.e. Extra XML and JPG files. It also can rename the files from the number specified so that all the XML and JPG files can be organised. Limitation The image files can only be in the JPG format. Limitation The name of the both counterparts (the image and its annotation XML) should have the same name for this script to work. This is the case for the files produced by labelimg, so if you're using that tool, it should be fine.","title":"Purpose"},{"location":"guides/utils/data-house-keeper/#how-it-works","text":"Once the script is started, it will go through the folder and find all the JPGs that have their XML files already present, it will match the names for doing so. If there are extra files present i.e. JPG files that have no matching XML files and vice versa, it will then ask if you want to delete the extra files, if the extra files are deleted only then it will proceed to the next step otherwise quit. If the extra files have been cleaned up (or there are no extra files) the script will ask if you want to rename the files, if you say yes then it will ask a number from which to start the numbering, this can be useful if you already have images in another folder and you just want to add more labelled data.","title":"How it works"},{"location":"guides/utils/data-house-keeper/#required-parameters","text":"Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above.","title":"Required Parameters"},{"location":"guides/utils/data-house-keeper/#example","text":"An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"Example"},{"location":"guides/utils/pascalvoc-to-csv/","text":"PascalVOC to CSV This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts. Purpose This script converts the image annotations/labellings from the PascalVOC (XML) format to CSV and writes them as a CSV file. Warning The script only reads the XML annotation files and converts them to a single CSV file for saving. It is essential that before running this script, that the folder has been cleaned up using the Data House Keeper script. How it works It will go through each XML file in the folder and get the respective labelled objects. Warning Since this script was purpose built for the table detection algorithm, it will set the labels of all the annotations to the same thing i.e. \"Table\". This script is not made for the case of multiple objects. It will assemble one big Pandas DataFrame that will contain the annotations from all of the XML files present. Finally, it will ask if you want to save the CSV file, if you say yes then it will ask for the name of the file. Please enter a valid name i.e. not empty to save the file. Do not enter the extension i.e. \".csv\" in the filename, that is understood. Required Parameters Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above. Example An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"PascalVOC to CSV"},{"location":"guides/utils/pascalvoc-to-csv/#pascalvoc-to-csv","text":"This script was written in order to aid the process of labelling data. At the time of writing this, the labelling tool of choice was LabelImg. It is a python utility that provides a nice GUI for labelling the images. One of the limitations of labelimg is that it does not support the labeling data in the CSV format which is required by the training scripts that were developed. In order to deal with the inconsistencies between the two formats, some scripts had to be written to keep everything clean and tidy. This is one of those scripts.","title":"PascalVOC to CSV"},{"location":"guides/utils/pascalvoc-to-csv/#purpose","text":"This script converts the image annotations/labellings from the PascalVOC (XML) format to CSV and writes them as a CSV file. Warning The script only reads the XML annotation files and converts them to a single CSV file for saving. It is essential that before running this script, that the folder has been cleaned up using the Data House Keeper script.","title":"Purpose"},{"location":"guides/utils/pascalvoc-to-csv/#how-it-works","text":"It will go through each XML file in the folder and get the respective labelled objects. Warning Since this script was purpose built for the table detection algorithm, it will set the labels of all the annotations to the same thing i.e. \"Table\". This script is not made for the case of multiple objects. It will assemble one big Pandas DataFrame that will contain the annotations from all of the XML files present. Finally, it will ask if you want to save the CSV file, if you say yes then it will ask for the name of the file. Please enter a valid name i.e. not empty to save the file. Do not enter the extension i.e. \".csv\" in the filename, that is understood.","title":"How it works"},{"location":"guides/utils/pascalvoc-to-csv/#required-parameters","text":"Following parameters are required to run the script: pascalvoc_path : This is the path to folder that contains the PascalVOC format annotations. Note The images and their corresponding XML annotation files should be in the same folder, the path to which is provided above.","title":"Required Parameters"},{"location":"guides/utils/pascalvoc-to-csv/#example","text":"An example on how to run the script is following: python utils/data-house-keeper.py --pascalvoc_path \"path/to/folder\"","title":"Example"}]}